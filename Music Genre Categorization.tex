% This file was converted to LaTeX by Writer2LaTeX ver. 1.4
% see http://writer2latex.sourceforge.net for more info
\documentclass[letterpaper]{article}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb,amsfonts,textcomp}
\usepackage{color}
\usepackage{array}
\usepackage{supertabular}
\usepackage{hhline}
\usepackage{hyperref}
\hypersetup{pdftex, colorlinks=true, linkcolor=blue, citecolor=blue, filecolor=blue, urlcolor=blue, pdftitle=, pdfauthor=MAHENDRA KUMAR, pdfsubject=, pdfkeywords=}
\usepackage[pdftex]{graphicx}
% Text styles
\newcommand\textstyleSubtitleChar[1]{\textcolor{black}{#1}}
\newcommand\textstyleappleconvertedspace[1]{#1}
% Outline numbering
\setcounter{secnumdepth}{0}
\makeatletter
\newcommand\arraybslash{\let\\\@arraycr}
\makeatother
% List styles
\newcounter{saveenum}
\newcommand\liststyleWWNumxxxii{%
\renewcommand\labelitemi{[F0B7?]}
\renewcommand\labelitemii{o}
\renewcommand\labelitemiii{[F0A7?]}
\renewcommand\labelitemiv{[F0B7?]}
}
\newcommand\liststyleWWNumxxxv{%
\renewcommand\labelitemi{[F0B7?]}
\renewcommand\labelitemii{o}
\renewcommand\labelitemiii{[F0A7?]}
\renewcommand\labelitemiv{[F0B7?]}
}
\newcommand\liststyleWWNumxxxiii{%
\renewcommand\labelitemi{[F0D8?]}
\renewcommand\labelitemii{o}
\renewcommand\labelitemiii{[F0A7?]}
\renewcommand\labelitemiv{[F0B7?]}
}
\newcommand\liststyleWWNumiii{%
\renewcommand\labelitemi{[F0B7?]}
\renewcommand\labelitemii{o}
\renewcommand\labelitemiii{[F0A7?]}
\renewcommand\labelitemiv{[F0B7?]}
}
\newcommand\liststyleWWNumiv{%
\renewcommand\labelitemi{[F0B7?]}
\renewcommand\labelitemii{o}
\renewcommand\labelitemiii{[F0A7?]}
\renewcommand\labelitemiv{[F0B7?]}
}
\newcommand\liststyleWWNumv{%
\renewcommand\labelitemi{[F0B7?]}
\renewcommand\labelitemii{o}
\renewcommand\labelitemiii{[F0A7?]}
\renewcommand\labelitemiv{[F0B7?]}
}
\newcommand\liststyleWWNumvi{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumvii{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumviii{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxi{%
\renewcommand\labelitemi{[F0B7?]}
\renewcommand\labelitemii{o}
\renewcommand\labelitemiii{[F0A7?]}
\renewcommand\labelitemiv{[F0B7?]}
}
\newcommand\liststyleWWNumxii{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\alph{enumii}}
\renewcommand\theenumiii{\roman{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxiv{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxiii{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\alph{enumii}}
\renewcommand\theenumiii{\roman{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxv{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxvi{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxvii{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\alph{enumii}}
\renewcommand\theenumiii{\roman{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxviii{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxix{%
\renewcommand\labelitemi{[F0B7?]}
\renewcommand\labelitemii{o}
\renewcommand\labelitemiii{[F0A7?]}
\renewcommand\labelitemiv{[F0B7?]}
}
\newcommand\liststyleWWNumxx{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxxi{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxxvii{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\alph{enumii}}
\renewcommand\theenumiii{\roman{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxxii{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxxiii{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxxiv{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxxv{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
\newcommand\liststyleWWNumxxvi{%
\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\theenumii{\arabic{enumii}}
\renewcommand\theenumiii{\arabic{enumiii}}
\renewcommand\theenumiv{\arabic{enumiv}}
\renewcommand\labelenumi{\theenumi.}
\renewcommand\labelenumii{\theenumii.}
\renewcommand\labelenumiii{\theenumiii.}
\renewcommand\labelenumiv{\theenumiv.}
}
% Page layout (geometry)
\setlength\voffset{-1in}
\setlength\hoffset{-1in}
\setlength\topmargin{1.27cm}
\setlength\oddsidemargin{2.54cm}
\setlength\textheight{21.690998cm}
\setlength\textwidth{16.509998cm}
\setlength\footskip{0.0cm}
\setlength\headheight{1.27cm}
\setlength\headsep{1.169cm}
% Footnote rule
\setlength{\skip\footins}{0.119cm}
\renewcommand\footnoterule{\vspace*{-0.018cm}\setlength\leftskip{0pt}\setlength\rightskip{0pt plus 1fil}\noindent\textcolor{black}{\rule{0.0\columnwidth}{0.018cm}}\vspace*{0.101cm}}
% Pages styles
\makeatletter
\newcommand\ps@Convertedi{
  \renewcommand\@oddhead{\thepage{}}
  \renewcommand\@evenhead{\@oddhead}
  \renewcommand\@oddfoot{}
  \renewcommand\@evenfoot{}
  \renewcommand\thepage{\arabic{page}}
}
\newcommand\ps@Standard{
  \renewcommand\@oddhead{\thepage{}}
  \renewcommand\@evenhead{\@oddhead}
  \renewcommand\@oddfoot{}
  \renewcommand\@evenfoot{}
  \renewcommand\thepage{\arabic{page}}
}
\makeatother
\pagestyle{Standard}
\setlength\tabcolsep{1mm}
\renewcommand\arraystretch{1.3}
% Non-floating captions
\makeatletter
\newcommand\captionof[1]{\def\@captype{#1}\caption}
\makeatother
\title{}
\author{MAHENDRA KUMAR}
\date{2016-05-21}
\begin{document}
\clearpage\setcounter{page}{1}\pagestyle{Standard}
{\centering
Music Genre Categorization
\par}

{\centering
Using Machine Learning Techniques
\par}


\bigskip


\bigskip

{\centering
\textit{A Major Project Report}
\par}

{\centering
\textit{Submitted in Partial Fulfilment for the Award of}
\par}

{\centering
\textbf{BACHELOR OF TECHNOLOGY}
\par}

{\centering
\textbf{IN}
\par}

{\centering
\textbf{COMPUTER SCIENCE ENGINEERING}
\par}


\bigskip

{\centering
\textbf{By}
\par}

{\centering
\textbf{Rahul Mishra }
\par}

{\centering
(2012ECS01)
\par}

{\centering
\textbf{Sumit Jha }
\par}

{\centering
(2012ECS11)
\par}

{\centering
\textbf{Mahendra Kumar }
\par}

{\centering
(2012ECS53)
\par}

{\centering
Under the Guidance of
\par}

{\centering
\textbf{Mr. Sanjay Sharma}
\par}


\bigskip

{\centering
\textbf{To\newline
}
\par}

\begin{figure}
\centering
\includegraphics[width=3.231cm,height=3.175cm]{MusicGenreCategorization-img/MusicGenreCategorization-img001.jpg}
\end{figure}

\bigskip


\bigskip


\bigskip

{\centering
\textbf{SHRI MATA VAISHNO DEVI UNIVERSITY, J\&K, INDIA}
\par}

{\centering
\textbf{MAY, 2016}
\par}

\clearpage\setcounter{page}{2}\pagestyle{Convertedi}
{\centering
\textbf{CERTIFICATE}
\par}


\bigskip


\bigskip


\bigskip

This is to certify that we, Rahul Mishra (2012ECS01), Sumit Jha (2012ECS11), Mahendra Kumar (2012ECS53) have worked
under the guidance of {}``Mr. Sanjay Sharma'' on the project titled \textbf{{}``Music Genre Categorization using
Machine Learnig Techniques''} in the School of Computer Science \& Engineering, College of Engineering, Shri Mata
Vaishno Devi University, Kakryal, Jammu \& Kashmir from 2\textsuperscript{nd} Jan 2016 to 23\textsuperscript{rd }May
2016 for the award of Bachelor of Technology in Computer Science \& Engineering.

The contents of this project, in full or in parts, have not been submitted to any other Institute or University for the
award of any degree or diploma.


\bigskip

Student 1 Signature\ \ :

Student 1 name\textbf{\ \ }:\textbf{\ \ Rahul Mishra}


\bigskip

Student 2 Signature\ \ :

Student 2 name\textbf{\ \ }:\textbf{\ \ Sumit Jha}


\bigskip

Student 3 Signature\ \ :

Student 3 name\textbf{\ \ }:\textbf{\ \ Mahendra Kumar}


\bigskip


\bigskip

This is to certify that the above student has worked for the project titled \textbf{{}``Music Genre Categorization using
Machine Learnig Techniques''} under my supervision.


\bigskip


\bigskip

{\centering
\textbf{Mr. Sanjay Sharma}
\par}

{\raggedleft
\textbf{Guide Name \& Signature}
\par}


\bigskip

{\raggedleft
Date: \_\_\_\_\_\_\_\_\_\_\_\_\_\_
\par}

{\centering
\textbf{ACKNOWLEDGEMENT}
\par}


\bigskip


\bigskip

\textcolor{black}{It is a moment of great pleasure and immense satisfaction for us to express our deepest sense of
gratitude and indebtedness to all the people who have contributed in making of our major project a rich experience.}


\bigskip

\textcolor{black}{We have taken efforts in this project. However, it would not have been possible without the kind
support and help of many individuals and organizations. We would like to extend our sincere thanks to all of them.}


\bigskip

\textcolor{black}{We are highly indebted to }\textbf{\textcolor{black}{Mr. Sanjay Sharma}}\textcolor{black}{ for his
guidance and constant supervision as well as for providing necessary information regarding the project \& also for his
support in completing the project.}


\bigskip

\textcolor{black}{We would like to express our gratitude towards members of }\textbf{\textcolor{black}{Shri Mata Vaishno
Devi University}}\textcolor{black}{ for their kind co-operation and encouragement which help us in completion of this
project.}


\bigskip

Finally, we would like to thank all our Teachers, Friends and Family Members who have supported us throughout and
enlightened us to take the right path and reach there. The days we spent in this institute will be cherished forever
and also be reckoned as a guiding factor in our career.


\bigskip


\bigskip

{\raggedleft
Rahul Mishra
\par}

{\raggedleft
Mahendra Kumar
\par}

{\raggedleft
Sumit Jha
\par}

{\raggedleft
B. Tech. \ 8\textsuperscript{th} Semester (SCSE)
\par}

\clearpage{\centering
\textbf{ABSTRACT}
\par}


\bigskip


\bigskip

Music has some really powerful effect on our emotions, and Human ear is quite incredible at predicting the genre
accurately, although music genre is a relatively complex concept that sometimes the music industry itself feels
confused in assigning genre to some of the songs.

Classification of genre by Machine is one of such technical approach which has a major drawback of predicting genre
accurately. Previously many attempts has been made to design system to categorize music but the prediction result was
not overwhelming. So, the problem continued that can machine predict genres of songs with better accuracy results?


\bigskip

The Project discusses various Machine Learning concepts of classification and improving the accuracy of those
classification techniques to be used for Genre based Music Categorization is the goal of our Project. In this project
Million Song Dataset, made available by LabROSA, containing 30 summary features for each music files one of 10 genre,
being used as training and testing data. The project focuses on implication of Classification algorithm like, Random
Forest, Gaussian Naive Bayes, Support Vector Machine, Decision tree, K-Nearest Neighbor. Lyrical modelling concept has
been the second approach improvising Bag of Words technique over the dataset.


\bigskip

We aimed to study and apply various machine learning concepts to solve a real world problem like Music Categorization
based on Genre and come out with an improved result of \textbf{62\% F1-score while using Multiclass classification
technique and 47\% F1-score accuracy while applying Lyrical Modelling concept}. For the fact that many songs are
unlabeled and much more songs are being released daily, the classification technique predicting genre with an accuracy
of 62\% F1-score is quite of help.


\bigskip

\clearpage
\textbf{List of Figures}


\bigskip

\listoffigures
\clearpage{\color{black}
\textbf{Table of Contents}}


\bigskip

\setcounter{tocdepth}{3}
\renewcommand\contentsname{}
\tableofcontents

\bigskip

\clearpage\section[INTRODUCTION]{\textbf{INTRODUCTION}}
\hypertarget{Toc451594937}{}
\bigskip

Classifying Music in CD stores and in online digital media has genre categorization at its base because of its ease to
differentiate music in varieties. It has been considered a tradition in CD stores to differentiate shelf on the basis
of genre to guide consumers into specific album of specific singer. Apart from that, Online digital media and personal
song collection also has genre as one of its main categorization criteria.

Humans are quite remarkable in distinguishing Music Genre. Usually a rough classification can be done by us after
listening few seconds of music but with the improvement in Machine Learning algorithms, after 2010 many attempts has
been made to design system for Automatic Music Genre Classification but the result were not very good. For those who
have used the same dataset of Million Song, provided by LabROSA, comprised of 10 different music genres, has yielded an
F1-sore of 58\% in [1],\textbf{ }which needs to be improved\textbf{.}


\bigskip

\subsection[\ Motivation]{\ Motivation}
\hypertarget{Toc451594938}{}Consumption of digital music is gaining popularity and its distribution over internet is
increasing by significant amount day by day. Music genre are generally used to categorize digital music collection so
as to facilitate navigation into it. Associating genres automatically to music has become so important firstly because
of the increase in number of music unit and secondly because automatic music genre classification generates category
independent of manual subjective category.

In this digital era, millions of songs are being accessed by consumers. Also, with the technological advancement artist
and producer are able to release and distribute songs instantly. This democratization of accessing digital media has
arisen the requirement to develop efficient categorization systems.


\bigskip

Humans have always been primary tool in attributing genre-tags to songs. Using machine to do the job is a more complex
task, but is the requirement of present time. Since Machine learning excels majorly in handling complex data very well,
so, this project addresses learning of various Machine learning algorithm to be used for automatically classifying
music files with improved prediction accuracy.


\bigskip

{\color{black}
\textbf{Advantages of Music Genre Categorization}}

Automatic classification based on genre will help creating music database in such a way that a general description like
90's classical is given by user and software based on the classifier does the file selection allowing user to create
playlist of his own selection criteria. \ Apart from its practical implication, music genre classification is
interesting field of study as well. Rigorous study of the classification and machine based music processing will
certainly enhance our knowledge about the human perception of music.\textcolor{black}{ }


\bigskip


\bigskip

\subsection[System Overview]{System Overview}
\hypertarget{Toc451594939}{}Extracting feature from music file for classification is not that easy. Digital music is
discretized sampled waveform, although categorization over frequency domain is preferred than the time domain because
of the ability of humans to differentiate between frequencies range like, deep bass in hip-hop is Low frequency while
talking, singing is mid-range frequency and sharp claps are considered high-pitched frequency.


\bigskip

The base of the project is Million Songs genre dataset that initially contains songs with 34 summary features given as
`genre, track id, artist name, title, loudness, tempo, time signature, key, mode, duration and 24 different timbre
value for each segment'. Each of the song is categorized in one of the 10 genres, mainly classic pop and rock,
classical, dance and electronica, folk, pop, hip-hop, jazz and blues, punk, metal \& lastly soul and reggae. After that
feature scaling has been applied over the dataset to obtain 30 summary feature namely `loudness, tempo, time signature,
key, mode, duration and 24 different timbre value for each segment' along with the `genre' for each song. The resultant
dataset has been modified into 9 genre collection by removing hip-hop. Before applying the machine learning algorithms
over dataset it has been accessed to obtain almost equal number of features for each genre (approx. 2000 songs of each
genre) type so as to provide a good training condition for Multiclass Classification algorithms. Some of the main
classification algorithms applied are Decision Tree classifier, K Nearest Neighbor classifier, Random Forest and Naïve
Bayes classifier, out of which Random Forest has given the best result of 62\% F1-Score. The machine learning algorithm
has been formulated using scikit learn library and numpy library of python programming.


\bigskip

Apart from that, the dataset has been modified into 2 summary feature dataset, one being genre and other the lyrics of
that particular song, for lyrical modelling approach to be applied over it. Based on the availability of lyrics for
each song in the `MSD' dataset for only approx. 24700 songs has been formed that has been re-featured to a collection
of approx. 7500 songs in 8 genres. In Lyrical Model the concept of `bag of words' has been implemented for
categorization.


\bigskip


\bigskip

\subsection[Scope and Result of Project]{Scope and Result of Project}
\hypertarget{Toc451594940}{}
\bigskip

{\color{black}
\textbf{Overview}}

This project involves evaluation of the current state of automatic genre based classification of music and represents
the methods of field with improvement. The work doesn't include the audio signal analysis process for feature
extraction but shows the importance and usage of the feature provided in dataset for prediction process. The projected
focuses specially on the algorithms applied on the features of song, its training, fitting of data and predicting the
genre based on the learned data. 


\bigskip


\bigskip

{\color{black}
\textbf{Structure}}

The project is represented in an organized way given as: The starting has included the overview of Music genre, the
Million Song dataset used and the feature detailing of the available features in the dataset. It has also enlisted the
human perception of music and study of genre categorization by human. The next Section consists of the Basics of
Machine Learning followed by various algorithm of multiclass classification and lyrical model approach that had been
used for the categorization purpose. The following section shows dataset and machine learning technique correlation for
fitting of data, learning from data and predicting the genre for the provided data. After that the next section
provides the experimental overview of the project, defining the implication and result for each of the used algorithm,
their advantages, drawbacks and comparison with one another to highlight the best model for classification purpose. The
next is all the codes written followed by Conclusion and lastly references that has been of great support in the whole
project. 


\bigskip

{\color{black}
\textbf{Result}}

Multiclass classification Algorithms prediction accuracy has resulted in 62\% F1-score, an increase of 4\% from the
previous best F1 Score of 58\% in [1], with the best model of Multiclass classification approach being Random Forest

An increase of 5\% with Lyrical Modelling concept with F1-score of 47\%.


\bigskip

\clearpage\section[Requirement Specification]{\textbf{Requirement Specification}}
\hypertarget{Toc451594941}{}
\bigskip

\subsection{Preprocessing the Data }
\hypertarget{Toc451594942}{}Before working on the dataset the data needs to be processed and formatted in suitable
formats and for that we require following steps to be followed before we can work on dataset.


\bigskip

\liststyleWWNumxxxii
\begin{itemize}
\item First of all, feature scaling needs to be performed over the dataset so that the features are scaled between
values (-1 to 1). So, that a single feature doesn't dominates the prediction process only because it has higher ranges
of value as compared to other features.
\item All the features are needed to be converted to floating types and not mixed type, because if latter is the case
numpy arrays generated by us, are converted to string type and that could lead to unexpected results.
\item In the Bag-of-word scheme, stemming should done before the bagging process as it could reduce the number of bags
and could produce more meaningful results.
\end{itemize}
\liststyleWWNumxxxv
\begin{itemize}
\item Another important preprocessing step that needs to be done is Principal Component Analysis (PCA) for dimension
reduction in bag-of-words so that we only consider important words only while classification.
\item Splitting the dataset into training and testing set is little tricky but is quite well handled by sklearn library
in a single line split them in 80:20 percentage respectively that too the elements are taken randomly to form the two
sets so that no biasing occurs.
\end{itemize}

\bigskip

\subsection{Python Libraries}
\hypertarget{Toc451594943}{}The few important libraries to be checked before running code on local environment.

\liststyleWWNumxxxiii
\begin{itemize}
\item Sklearn (scikit-learn)
\item Numpy
\item Scipy
\item Matplotlib
\item Beautifulsoup
\item Request
\item Pickel
\end{itemize}

\bigskip


\bigskip

\clearpage\section[Music Genre Classification]{\textbf{Music Genre Classification}}
\hypertarget{Toc451594944}{}
\bigskip

\textcolor{black}{A common definition of terms is necessary in every field of science and that too is applicable for
Music genre classification. However, such definition doesn't happen to exist in our field of study, even the literature
disagrees on the basic terminologies. Two of the main reasons identified for this are:}

\liststyleWWNumiii
\begin{itemize}
\item Music being a part of daily life, has terms having an intuitive meaning, like for the features of music song such
as tempo, rhythm, pitch.
\item Human perceive sound depending on its personal, cultural and emotional aspect and so does happens for
classification.
\end{itemize}

\bigskip

This lack of simple basic hinders the progress of audio signal classification resulting in degradation in feature
extraction on which the whole classification process majorly relies. 

Music genre classification is a subfield of the larger field of audio signal classification, defined as working with the
extracted features of audio signal (obtaining relevant features from a sound and based on those identify the set of
class the sound is most likely to be classified) to be used by the classification algorithms for genre
\textcolor{black}{. Automatic genre categorization refers to achieve this task with machines using machine learning
techniques.}


\bigskip

\subsection[Genres and Feature vector]{Genres and Feature vector}
\hypertarget{Toc451594945}{}Conventional classification approach that identifies music belonging to a particular
tradition or set of conventions is \textbf{Music genre }[2]. Now days genre has become the very basic categorization
criteria and so the music are categorized into different genres but because of the \ artistic nature of music these
criteria of classification is considered subjective and it might happen that some genres may overlap.

Although the project doesn't focuses on the feature extraction technique but the study is really the need for the
feature selection to be done from dataset so as to achieve better prediction result. Apart from that based on the
features availability the fitting of dataset for training purpose of the classifiers used throughout the project.


\bigskip

\subsubsection{Genre}
\hypertarget{Toc451594946}{}{\color{black}
\textbf{History}}

\textbf{\ \ }Initially genre analysis has been used for teaching and learning of English for special purpose, but the
digitalization of communication and domination of Internet after 90's has extended genre study to the digital field.
Crowsten and William (1998) were some of the first to study genre for digital media. They identified the importance of
study of genre for analysing digital world of internet because of ease of access [3]. \textcolor{black}{They identified
48 various genre after studying random 100 web pages.}


\bigskip

The term `Cybergenre' was first coined by Shepherd and Watters in 2001, denoting digital genre which was further termed
in two categories based on the dependency on the media by many authors.

\liststyleWWNumiv
\begin{itemize}
\item Extant subgenre: genre of some media that migrated to another computer environment and replicated without
exploiting the capabilities of the new media, e.g. newspaper, dictionaries, research articles and biographies.
\item Novel genres: genres totally dependent on one medium and couldn't exist in another, e.g. virtual games and
homepages.
\end{itemize}

\bigskip

{\color{black}
\textbf{Genre in Music }}

The Latin word `genus' meaning `class or kind' was the origin for word genre that we use today. It might be a considered
a category that can be defined by structural or functional criteria. Sometimes criteria like musical techniques, style,
the cultural context or the content of the music also plays important role to define a music genre or subgenre.
Geographical origin also sometimes play important criteria to classify a music genre like `jazz and blues', though a
single geographic category might include many subgenres.


\bigskip

Genre classification is always been a subjective term in respect of both listener and socio-cultural environment. Hence
a definition for music genre can be stated as: `\textit{A music genre is a specific class of music consisting of set of
some common properties that an average listener perceive so as to distinguish music of that class from other songs'. }A
specific genre has unique characteristic feature of rhythmic structure as well as the instrumentation property for that
particular kind of music, although there are certain more features used human for manual categorization of music genre.
The major challenge of automatic genre classification is to find out those factors and extracting those features from
the music file.


\bigskip

\subsubsection[Feature Extraction]{Feature Extraction}
\hypertarget{Toc451594947}{}This includes the very basic key concept of feature extraction keeping signal processing in
its background. The project doesn't implement any technique for feature extraction but prior knowledge of features of
songs that gets extracted from song is needed before going to the implementation part of machine learning classifier
algorithm so as to learn the relative feature to be used in the training process. The main difficulty of music genre
categorization at first is to differentiate between music styles, i.e. to obtain feature vector for various genre song.
It is similar to the fact of comparing two object (in our case it is music songs) for their similarity and
dissimilarity when they can't be directly compared. Although our project directly access the million song dataset that
already contains extracted features i.e. 34 summary feature that can be directly accessed only after separating the
required features from dataset and applying feature scaling.


\bigskip

\textit{Feature Extraction} is a process of transforming the data in order to make it accessible for essential
information retrieval by computing a numerical representation for a segment of audio signal. For classification
\textit{feature extraction} is one of the two commonly used pre-processing technique. It applies one or more
transformation over raw data to generate new features. The other one is \textit{feature selection} -- that identifies a
feature subset within the raw data to be used for effective classification which has been implemented within the
project. The Feature selection process can be applied over the input dataset or over the output provided by Feature
extraction process. A classification system might use both or either of the two techniques depending on the accuracy
the feature extraction is giving as shown in \textit{Fig} 1. Our project uses only the Feature selection pre-processing
as the Million songs dataset initially contains the extracted features of songs in it. The Feature selection
pre-processing technique has been elaborated further in the project.


\bigskip

 \includegraphics[width=15.558cm,height=2.381cm]{MusicGenreCategorization-img/MusicGenreCategorization-img002.png} 

\captionof{figure}{\hypertarget{Toc451594966}{}Schematic overview of feature extraction and feature selection}

\bigskip

Most of the information of audio signal is encoded in its frequency phase and the spectral component's amplitude. So,
the information can be retrieved by examining the signal's \textit{frequency spectrum }that is similar to what happens
in human auditory system. `\textit{Fourier Analysis}{}' is a mathematical technique used for the above. \textit{Fourier
transform, Short-Time Fourier Transform} are some of Fourier analysis implementation method. For more detailing, one
can consult books on signal processing.


\bigskip

\subsubsection[Common Music Genres]{Common Music Genres}
\hypertarget{Toc451594948}{}From rock to pop, from classical to hip-hop, music ranges in various type and styles. There
is a very rich history of music genres that even contain enlightening geographical significance. This section
introduces to some of the common genres that has been used for the classification purpose in the project.


\bigskip

\liststyleWWNumv
\begin{itemize}
\item \textbf{Classical pop and rock}
\end{itemize}
Classical rock is a radio format that has its origin in 1980s from the album-oriented rock format. It contains minimal
voice component and is traditionally built on simple unsyncopated or unmodified rhythms.

Pop music or `popular music' (terms being used interchangeably) has originated from the western world during 1950s and
60s. Music of this genre is generally featured by a consistent rhythmic element. Although rock and pop often have
overlaps. Classical music do not contain repetitive and heavily pronounced rhythms and has less bass.


\bigskip

\liststyleWWNumv
\begin{itemize}
\item \textbf{Dance and electronica}
\end{itemize}
Also referred as Electronic Dance Music (EDM) is a kind of club music having broad range of persuasive electronic music.
It has some subgenres namely Disco, Techno, House, Ambient, jungle, drum and bass and Electro. It has consistent bass
rhythms i.e. all the quarter notes.


\bigskip

\liststyleWWNumv
\begin{itemize}
\item \textbf{Folk}
\end{itemize}
It is combination of both traditional music and the folk revival of 20\textsuperscript{th} century that evolved from the
traditional music itself. It is mainly geographically originated genre and is often related to a nation's culture. A
folk music has a short instrumental piece called tune, a melody and repetitive word section.


\bigskip

\liststyleWWNumv
\begin{itemize}
\item \textbf{Hip-hop}
\end{itemize}
Hip-hop music also termed `rap' has origin in 1970s in the United States. This genre consist of stylized rhythmic music
i.e. a rhythmic and rhyming speech usually accompanies rapping. It is generally associated with deep, rhythmically
repetitive bass.


\bigskip

\liststyleWWNumv
\begin{itemize}
\item \textbf{Jazz and blues}
\end{itemize}
Jazz and blues music was originated from African American communities during late 19\textsuperscript{th} and early
20\textsuperscript{th} century. This genre can refer either to blues containing advanced harmonies and rhythms or to
jazz having rhythms real simple. This categorization genre have soft volume and excludes repetitive rhythms.


\bigskip

\liststyleWWNumv
\begin{itemize}
\item \textbf{Metal}
\end{itemize}
Metal or mostly called heavy metal is a genre of rock music have origin in UK and USA in between 1960s and 70s. Loud
distorted guitars is metal genre's traditional characteristics. It is often characterized by emphatic music dense bass
\& drum sounds along with energized vocals.


\bigskip

\liststyleWWNumv
\begin{itemize}
\item \textbf{Punk}
\end{itemize}
This is a loud, fast and kind of distorted genre category that has its origin in US during 1970s. It hard edged melodies
and stripped down instrumentation.


\bigskip

\liststyleWWNumv
\begin{itemize}
\item \textbf{Soul and reggae}
\end{itemize}
Reggae music genre has its origin during 1960s in Jamaica. The bass sound is often thick and heavy in this genre. The
bass sound is equalized too so that the upper frequencies are removed and emphasized lower frequency is obtained.


\bigskip

\subsection[Schematic Model]{Schematic Model}
\hypertarget{Toc451594949}{}As stated previously, Music genre categorization is all about classification of songs into
some predefined categories. The goal is therefore to design algorithms that classifies the input digital music file
into specific genre given the features of the song is provided with the song initially.

{\centering 
\includegraphics[width=13.547cm,height=5.556cm]{MusicGenreCategorization-img/MusicGenreCategorization-img003.jpg} \par}
\captionof{figure}{\hypertarget{Toc451594967}{}Schematic Overview of music genre Classification}

\bigskip

The oval shape is the Million Song dataset that already contains the extracted featured of the songs and so the process
of extraction over audio file is not required. However, the process has been discussed in the section \textbf{2.1.2.
}The only requirement to achieve goal is to do apply pre-processing technique feature selection (Section 2.1.2.) and
then applying the classifier over the selected feature to do the final classification of songs into genres. The
classification has been well deduced in Section 3. \textit{Figure 2}. Shows the most simple representation of the
classification been done.


\bigskip

\subsection[Difficulty]{Difficulty}
\hypertarget{Toc451594950}{}Music genre classification is a tough task to perform with machine being the classifying
tool. This is mostly because of some of following reasons:

First of all, it is a fact that music is a perceptual phenomenon. Although human ear receives the sound well but how it
is actually perceived by us is still a major topic of discussion and has no clear results. The knowledge we have about
perception is not sufficient enough to build a system that perfectly simulates like human for music perception.

\ \ Secondly, the human perception of a song genre is not always correct and hence the music genre classification by
human doesn't always result correct. So, it is difficult to train a system for classification purpose in presence of
such ambiguity.

So, to model a classification system, it might be a need to check if the system too does the mistakes in classifying
particular genre of song that can be classified into more than one genre, similarly like the humans do.


\bigskip

\subsection[The Dataset and Features]{The Dataset and Features}
\hypertarget{Toc451594951}{}For this project the Million Song Genre Dataset has been used, that is been made available
by LabROSA at Columbia university [4], for educational study purpose.


\bigskip



\begin{figure}
\centering
\includegraphics[width=8.55cm,height=8.091cm]{MusicGenreCategorization-img/MusicGenreCategorization-img004.png}
\end{figure}
This dataset consists of about million songs of various genres, to be precise of 10 genres, namely classic pop and rock,
classical, dance and electronica, folk, pop, hip-hop, jazz and blues, punk, metal \& lastly soul and reggae. The
distribution of songs into various genres can be recognized in Figure 3.


\bigskip

\captionof{figure}{\hypertarget{Toc451594968}{}Dataset Composition}

\bigskip


\bigskip

This dataset is a well-organized classification dataset as it consist the features of each of the song in the dataset
too. The Million Song Dataset has about million songs with related metadata and audio analysis features, in total 34
summary features given as `genre, track id, artist name, title, loudness, tempo, time signature, key, mode, duration
and 24 different timbre value for each segment'. The Feature representation and value of each feature for some of the
songs can be viewed in the \textit{Figure 4}.

In this dataset, we have each song as a training example containing 34 summary features along with the genre of the
track, which would be of use during training as well as testing the classifiers for predicting the genre of particular
combination of features.

 \includegraphics[width=15.901cm,height=14.605cm]{MusicGenreCategorization-img/MusicGenreCategorization-img005.jpg} 

\captionof{figure}{\hypertarget{Toc451594969}{}Small part of Million Songs Genre Dataset}
\subsubsection[Terminologies]{Terminologies}
\hypertarget{Toc451594952}{}
\bigskip

The dataset contains various features that needs little description.

\textbf{genre:\ \ \ \ }is the music category of the particular track in the dataset.

\textbf{track\_id:\ \  \ \ }is the musicmatch track id of the track

\textbf{artist\_name:\ \ }is the song's artist name

\textbf{title:\ \ \ \ }title of the song

\textbf{loudness:}\ \ is the property of a sound that is primarily a psychological correlation of physical strength
(amplitude).

\textbf{tempo:}\ \ The tempo is the speed of the underlying beat for a piece of music. Tempo is measured in BPM, or
Beats/Minute. The top number represents how many beats there are in a measure, and the bottom number represents the
note value which makes one beat.

\textbf{key:}\ \ The key of a piece is a group of pitches, or scale upon which a music composition is created.

\textbf{mode:}\ \ Refers to a type of scale, coupled with a set of characteristic melodic behaviors.

\textbf{duration:}\ \ song duration (in Secs).

\textbf{avg\_timber1 - 12:\ \ }Timbre is then a general term for the distinguishable characteristics of a tone.

\textbf{var\_timber1 -- 12:\ \ }Timbre is mainly determined by the harmonic content of a sound and the dynamic
characteristics.


\bigskip


\bigskip


\bigskip


\bigskip

\subsubsection[Feature Selection and implementation]{Feature Selection and implementation}
\hypertarget{Toc451594953}{}\textstyleSubtitleChar{\textbf{For Multiclass Classification }}

The given dataset composition of songs is unbalanced as shown in \textit{Figure 3}, that classifies genres with 40\%
prediction rate except for `classical rock and pop', since this genre is 40.09\% of the whole dataset, which is quite a
large composition, that enables the machine to train better over `classical rock and pop' and give a good testing
result.

\begin{figure}
\centering
\includegraphics[width=6.999cm,height=7.294cm]{MusicGenreCategorization-img/MusicGenreCategorization-img006.jpg}
\end{figure}
So, we redistributed the dataset to obtain a balanced composition of songs given in \textit{Figure 5}. 


\bigskip

\captionof{figure}[. Balanced Dataset Composition]{\hypertarget{Toc451594970}{}. Balanced Dataset Composition}
\ \ Apart from that, the feature selection process has been implemented, to select important features out of the given
34 summary feature in the dataset corresponding to each track, and speparating genre form the features. In the project
we have used 30 summary features namely `loudness, tempo, time signature, key, mode, duration and 24 different timbre
value for each segment{}' appended in a single vector termed feature vector along with genre for each track in the
balanced dataset.


\bigskip

{\color{black}
\textbf{Code Implementation}}

\liststyleWWNumvi
\begin{enumerate}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~numpy~as~np~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~math~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~sys~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~matplotlib~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.multiclass~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~OneVsRestClassifier~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.svm~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~LinearSVC~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.cross\_validation~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~train\_test\_split~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~time~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~tree~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.metrics~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~accuracy\_score~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.grid\_search~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~GridSearchCV~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.metrics~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~confusion\_matrix~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.ensemble~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~RandomForestClassifier~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.neighbors~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~KNeighborsClassifier~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.naive\_bayes~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~GaussianNB~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.ensemble~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~AdaBoostClassifier~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.metrics~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~classification\_report~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.preprocessing~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~MinMaxScaler~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.decomposition~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~RandomizedPCA~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#sys.stdout~=~open('output.txt',~'w')}\textcolor{black}{~~}
\item \textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{def}}\textcolor{black}{~HPOptimizationGridSearch(feature\_train,label\_train):~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Fitting~the~classifier~to~the~training~set{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~~~~t0~=~time.time()~~}
\item \textcolor{black}{~~~~param\_grid~=~\{~~}
\item \textcolor{black}{~~~~~~~~~~~~~}\textcolor{blue}{{}'min\_samples\_split'}\textcolor{black}{:~range(1,41),~~}
\item \textcolor{black}{~~~~~~~~~~~~~~}\textcolor{blue}{{}'min\_samples\_leaf'}\textcolor{black}{:~range(1,20),~~}
\item \textcolor{black}{~~~~~~~~~~~~~~}\textcolor{blue}{{}'n\_estimators'}\textcolor{black}{:~range(1,500)~~}
\item \textcolor{black}{~~~~~~~~~~~~~~\}~~}
\item \textcolor{black}{~~~~clf~=~GridSearchCV(OneVsRestClassifier(RandomForestClassifier()),~param\_grid)~~}
\item \textcolor{black}{~~~~clf~=~clf.fit(feature\_train,label\_train)~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}done~in~\%0.3fs{\textquotedbl}}\textcolor{black}{~\%~(time.time()~-~t0)~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Best~estimator~found~by~grid~search:{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~clf.best\_estimator\_~~}
\item \textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{def}}\textcolor{black}{~processItem(datapoint):~~}
\item \textcolor{black}{~~~~~~}
\item \textcolor{black}{~~~~info=[datapoint[0]]~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~feature~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~datapoint[4:]:~~}
\item \textcolor{black}{~~~~~~~~info.append(float(feature))~~}
\item \textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{return}}\textcolor{black}{~info~~}
\item \textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{def}}\textcolor{black}{~targetFeatureSplit(~data~):~~}
\item
\textcolor{black}{~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{{\textquotedbl}{\textquotedbl}{\textquotedbl}~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~given~a~numpy~array~like~the~one~returned~from}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~featureFormat,~separate~out~the~first~feature}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~and~put~it~into~its~own~list~(this~should~be~the~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~quantity~you~want~to~predict)}\textcolor{black}{~}
\item \textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~return~targets~and~features~as~separate~lists}\textcolor{black}{~}
\item \textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~(sklearn~can~generally~handle~both~lists~and~numpy~arrays~as~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~input~formats~when~training/predicting)}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~{\textquotedbl}{\textquotedbl}{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~~~~target~=~[]~~}
\item \textcolor{black}{~~~~features~=~[]~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~item~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~data:~~}
\item \textcolor{black}{~~~~~~~~target.append(~item[0]~)~~}
\item \textcolor{black}{~~~~~~~~features.append(~item[1:]~)~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{return}}\textcolor{black}{~target,~features~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{genres=\{\}~~}
\item \textcolor{black}{data\_list=[]~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{def}}\textcolor{black}{~done():~~}
\item
\textcolor{black}{~~~~line\_num=0~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#line~number~in~text~file}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~with~open(}\textcolor{blue}{{}'msd\_genre\_dataset.txt'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~as~f:~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~line~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~f:~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{~line\_num{\textless}10:~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{pass}}\textcolor{black}{~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{else}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~temp=line.split(}\textcolor{blue}{{}','}\textcolor{black}{)~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{try}}\textcolor{black}{:~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{(genres[temp[0]]~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{and}}\textcolor{black}{~genres[temp[0]]{\textless}2000):~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{(temp[0]==}\textcolor{blue}{{\textquotedbl}hip-hop{\textquotedbl}}\textcolor{black}{):~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~temp[0]=}\textcolor{blue}{{}'pop'}\textcolor{black}{~~~~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~genres[temp[0]]+=1~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~data\_list.append(processItem(temp))~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{except}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~genres[temp[0]]=1~~}
\item \textcolor{black}{~~~~~~~~~~~~line\_num+=1~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}**************************************************************************{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}No~of~songs:~{\textquotedbl}}\textcolor{black}{,line\_num-10~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}**************************************************************************{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Dataset~composition:{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~type~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~genres.keys():~~}
\item
\textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#print~{\textquotedbl}{\textbackslash}{\textquotedbl}{\textquotedbl}+~type+{\textquotedbl}{\textbackslash}{\textquotedbl},{\textquotedbl},}\textcolor{black}{~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~type,genres[type]~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}**************************************************************************{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Loading~data~in~numpy~arrays.{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~~~~t=time.time()~~}
\item \textcolor{black}{~~~~np\_data=np.array(data\_list)~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Time~elapsed:{\textquotedbl}}\textcolor{black}{,time.time()-t,}\textcolor{blue}{{\textquotedbl}secs.{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~~~~t=time.time()~~}
\item \textcolor{black}{~~~~scaler=MinMaxScaler()~~}
\item \textcolor{black}{~~~~np\_data[:,1:]=scaler.fit\_transform(np\_data[:,1:])~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Splitting~data~into~target~and~features{\textquotedbl}}\textcolor{black}{~~~}
\item \textcolor{black}{~~~~labels,features=targetFeatureSplit(np\_data)~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Time~elapsed:{\textquotedbl}}\textcolor{black}{,time.time()-t,}\textcolor{blue}{{\textquotedbl}secs.{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Creating~training~set~and~Test~set{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~~~~t=time.time()~~}
\item
\textcolor{black}{~~~~feature\_train,feature\_test,label\_train,label\_test~=~train\_test\_split(features,~labels,~test\_size=0.20,~random\_state=42)~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Time~elapsed:{\textquotedbl}}\textcolor{black}{,time.time()-t,}\textcolor{blue}{{\textquotedbl}secs.{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{blue}{{\textquotedbl}Fitting~and~predicting~the~data{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~~~~t=time.time()~~}
\item \textcolor{black}{~~}
\item
\textcolor{black}{~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#HPOptimizationGridSearch(feature\_train,label\_train)}\textcolor{black}{~~}
\item \textcolor{black}{~~~~~~}
\item
\textcolor{black}{~~~~clf=OneVsRestClassifier(RandomForestClassifier(min\_samples\_split=2,min\_samples\_leaf=1,n\_estimators=300))~~}
\item \textcolor{black}{~~~~clf.fit(feature\_train,label\_train)~~}
\item \textcolor{black}{~~~~pred=clf.predict(feature\_test)~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Time~elapsed:{\textquotedbl}}\textcolor{black}{,time.time()-t,}\textcolor{blue}{{\textquotedbl}secs.{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}accuracy\_score={\textquotedbl}}\textcolor{black}{,accuracy\_score(label\_test,pred)~~}
\item \textcolor{black}{~~~~t=time.time()~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}confusion\_matrix:{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{(classification\_report(label\_test,pred))~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}classification\_report:{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~classification\_report(label\_test,pred)~~}
\item \textcolor{black}{~~~~~~}
\item \textcolor{black}{done()}
\end{enumerate}

\bigskip

{\color{black}
\textbf{For Lyrical Analysis}}

The technique works on the lyrics of tracks. So, based on the availabilty of songs [5], dataset for approx 24600 was
built which again was baanced based on lyrics availabilty for each genre and a new dataset composition was achieved.
From the balacned dataset lyrics for each track is used as feature along with the genre, so as to apply bag of words
technique. The dataset overview after creating bag of words is shown in \textit{Fig 6.}

 \includegraphics[width=15.914cm,height=10.455cm]{MusicGenreCategorization-img/MusicGenreCategorization-img007.png} 

\captionof{figure}[Pickel of Lyrical Dataset]{\hypertarget{Toc451594971}{}Pickel of Lyrical Dataset}
{\color{black}
\textbf{Code for Obtaining Lyrics for the tracks in the dataset}}

\liststyleWWNumvii
\begin{enumerate}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~requests~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~bs4~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~BeautifulSoup,~Comment,~NavigableString~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~sys,~codecs,~json~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{def}}\textcolor{black}{~getLyrics(singer,~song):~~}
\item \textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#Replace~spaces~with~\_}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~singer~=~singer.replace(}\textcolor{blue}{{}'~'}\textcolor{black}{,~}\textcolor{blue}{{}'\_'}\textcolor{black}{)~~}
\item
\textcolor{black}{~~~~~~~~song~=~song.replace(}\textcolor{blue}{{}'~'}\textcolor{black}{,~}\textcolor{blue}{{}'\_'}\textcolor{black}{)~~}
\item
\textcolor{black}{~~~~~~~~r~=~requests.get(}\textcolor{blue}{{}'http://lyrics.wikia.com/\{0\}:\{1\}'}\textcolor{black}{.format(singer,song))~~}
\item \textcolor{black}{~~~~~~~~s~=~BeautifulSoup(r.text,}\textcolor{blue}{{}'lxml'}\textcolor{black}{)~~}
\item \textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#Get~main~lyrics~holder}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~lyrics~=~s.find(}\textcolor{blue}{{\textquotedbl}div{\textquotedbl}}\textcolor{black}{,\{}\textcolor{blue}{{}'class'}\textcolor{black}{:}\textcolor{blue}{{}'lyricbox'}\textcolor{black}{\})~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{~lyrics~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{is}}\textcolor{black}{~None:~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#raise~ValueError({\textquotedbl}Song~or~Singer~does~not~exist~or~the~API~does~not~have~Lyrics{\textquotedbl})}\textcolor{black}{~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{return}}\textcolor{black}{~None~~}
\item \textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#Remove~Scripts}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~[s.extract()~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~s~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~lyrics(}\textcolor{blue}{{}'script'}\textcolor{black}{)]~~}
\item \textcolor{black}{~~~~~~~}
\item \textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#Remove~Comments}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~comments~=~lyrics.findAll(text=}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{lambda}}\textcolor{black}{~text:isinstance(text,~Comment))~~}
\item
\textcolor{black}{~~~~~~~~[comment.extract()~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~comment~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~comments]~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#Remove~unecessary~tags}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~tag~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~[}\textcolor{blue}{{}'div'}\textcolor{black}{,}\textcolor{blue}{{}'i'}\textcolor{black}{,}\textcolor{blue}{{}'b'}\textcolor{black}{,}\textcolor{blue}{{}'a'}\textcolor{black}{]:~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~match~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~lyrics.findAll(tag):~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~match.replaceWithChildren()~~}
\item
\textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#Get~output~as~a~string~and~remove~non~unicode~characters~and~replace~{\textless}br{\textgreater}~with~newlines}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~lyrics=~str(lyrics).replace(}\textcolor{blue}{{}'{\textbackslash}n'}\textcolor{black}{,}\textcolor{blue}{{}'{}'}\textcolor{black}{).replace(}\textcolor{blue}{{}'{\textless}br/{\textgreater}'}\textcolor{black}{,}\textcolor{blue}{{}'~'}\textcolor{black}{)~~}
\item \textcolor{black}{~~~~~~~~output=lyrics[22:-6:]~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{try}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{return}}\textcolor{black}{~output~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{except}}\textcolor{black}{:~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{return}}\textcolor{black}{~output.encode(}\textcolor{blue}{{}'utf-8'}\textcolor{black}{)~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{genres=\{\}~~}
\item \textcolor{black}{lyrics\_found=0~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~codecs~~}
\item
\textcolor{black}{y=codecs.open(}\textcolor{blue}{{\textquotedbl}songLyrics1.txt{\textquotedbl}}\textcolor{black}{,}\textcolor{blue}{{\textquotedbl}w{\textquotedbl}}\textcolor{black}{)~~}
\item \textcolor{black}{line\_num=0~~}
\item
\textcolor{black}{with~codecs.open(}\textcolor{blue}{{}'msd\_genre\_dataset.txt'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~as~f:~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~line~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~f:~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{~line\_num{\textless}=27432:~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{pass}}\textcolor{black}{~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{else}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~temp=line.split(}\textcolor{blue}{{}','}\textcolor{black}{)~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#print~{\textquotedbl}artist~name={\textquotedbl},temp[2],{\textquotedbl}Title={\textquotedbl},temp[3]}\textcolor{black}{~~}
\item \textcolor{black}{~~~~~~~~~~~~lyrics=getLyrics(temp[2],temp[3])~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{(lyrics!=None):~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{(lyrics.split(}\textcolor{blue}{{}'~'}\textcolor{black}{)[0]!=}\textcolor{blue}{{}'{\textless}span'}\textcolor{black}{):~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{try}}\textcolor{black}{:~~~~~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~genres[temp[0]]+=1~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{except}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~genres[temp[0]]=1~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~lyrics\_found+=1~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~y.write(temp[0]+}\textcolor{blue}{{}'**/**'}\textcolor{black}{+lyrics)~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~y.write(}\textcolor{blue}{{}'{\textbackslash}n'}\textcolor{black}{)~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{(line\_num\%100==0):~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}At~present~progress{\textbackslash}n{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Songs~processed:~{\textquotedbl}}\textcolor{black}{+~str(line\_num)~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Lyrics~saved~upto~now:{\textquotedbl}}\textcolor{black}{~+str(lyrics\_found)~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~gen~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~genres.keys():~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~gen,}\textcolor{blue}{{\textquotedbl}:{\textquotedbl}}\textcolor{black}{,genres[gen]~~}
\item \textcolor{black}{~~~~~~~~line\_num+=1~~}
\item \textcolor{black}{y.close()~~}
\end{enumerate}

\bigskip

{\color{black}
\textbf{Code for obtaining bag of words from Pickel Data}}

\liststyleWWNumviii
\begin{enumerate}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~\_\_future\_\_~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~division~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~nltk.corpus~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~stopwords~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~nltk.stem~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~SnowballStemmer~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~unicodedata~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~codecs~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~string~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~pickle~~}
\item \textcolor{black}{stemmer~=~SnowballStemmer(}\textcolor{blue}{{}'english'}\textcolor{black}{)~~}
\item
\textcolor{black}{cachedStopWords~=~stopwords.words(}\textcolor{blue}{{\textquotedbl}english{\textquotedbl}}\textcolor{black}{)~~}
\item \textcolor{black}{data=[]~~}
\item
\textcolor{black}{with~codecs.open(}\textcolor{blue}{{}'finalLyricsList2.txt'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~as~f:~~}
\item \textcolor{black}{~~~~lines=f.readlines()~~}
\item \textcolor{black}{~~~~count=0~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~line~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~lines:~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~~~~~~~labels,features=line.split(}\textcolor{blue}{{}'**/**'}\textcolor{black}{)~~~~~~~}
\item
\textcolor{black}{~~~~~~~~features~=~unicode(features,~}\textcolor{blue}{{\textquotedbl}utf-8{\textquotedbl}}\textcolor{black}{)~~}
\item
\textcolor{black}{~~~~~~~~features~=~unicodedata.normalize(}\textcolor{blue}{{}'NFKD'}\textcolor{black}{,features).encode(}\textcolor{blue}{{}'ascii'}\textcolor{black}{,}\textcolor{blue}{{}'ignore'}\textcolor{black}{)~~}
\item \textcolor{black}{~~~~~~~~features=features.translate(None,~string.punctuation)~~}
\item \textcolor{black}{~~~~~~~~features=features.lower()~~~~}
\item
\textcolor{black}{~~~~~~~~features=[word~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~word~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~features.split()~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{~word~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{not}}\textcolor{black}{~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~cachedStopWords]~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~word~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~features:~~}
\item \textcolor{black}{~~~~~~~~~~~~stemmer.stem(word)~~}
\item
\textcolor{black}{~~~~~~~~features=}\textcolor{blue}{{\textquotedbl}~{\textquotedbl}}\textcolor{black}{.join(features)~~}
\item \textcolor{black}{~~~~~~~~data.append([labels,features])~~}
\item \textcolor{black}{~~~~~~~~count+=1~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{(count\%500==0):~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}completed:{\textquotedbl}}\textcolor{black}{,count/len(lines)*100~~}
\item \textcolor{black}{~~~~~~~~~}
\item \textcolor{black}{~~~~genres=\{\}~~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~line~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~lines:~~}
\item \textcolor{black}{~~~~~~~~labels,features=line.split(}\textcolor{blue}{{}'**/**'}\textcolor{black}{)~~~~~~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{try}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~genres[labels]+=1~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{except}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~genres[labels]=0~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}File~composition{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~g~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~genres.keys():~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~g+}\textcolor{blue}{{\textquotedbl}:~{\textquotedbl}}\textcolor{black}{,genres[g]~~}
\item \textcolor{black}{~~~~~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#saving~into~serialized~object}\textcolor{black}{~~}
\item
\textcolor{black}{stem\_data=open(}\textcolor{blue}{{\textquotedbl}stemmed\_text{\textquotedbl}}\textcolor{black}{,}\textcolor{blue}{{}'wb'}\textcolor{black}{)~~}
\item \textcolor{black}{pickle.dump(data,stem\_data)~~}
\item \textcolor{black}{stem\_data.close()~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.feature\_extraction.text~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~CountVectorizer~~}
\item \textcolor{black}{vectorizer=CountVectorizer()~~}
\item \textcolor{black}{email=[]~~}
\item \textcolor{black}{bow=vectorizer.fit(email)~~}
\item \textcolor{black}{bow=vectorizer.transform(email)~~}
\item
\textcolor{black}{vectorizer.vocabulary\_.get(}\textcolor{blue}{{\textquotedbl}great{\textquotedbl}}\textcolor{black}{)~~}
\end{enumerate}

\bigskip

\clearpage\section[Machine Learning]{\textbf{Machine Learning}}
\hypertarget{Toc451594954}{}
\bigskip

\subsection[Role of Machine Learning ]{Role of Machine Learning }
\hypertarget{Toc451594955}{}Machine learning is a subpart of computer science and engineering that developed and emerged
from the study of pattern recognition and computational learning theory of programmable machines in AI (Artificial
Intelligence). In late 20\textsuperscript{th} century, Arthur Samuel coined the term of machine learning as a
{\textquotedbl}Field of study that gives computers the ability to learn without being explicitly
programmed{\textquotedbl}. Machine learning techniques identifies and researches the study and development of
principles and theorem that can learn from and make prognostication on data. Such procedures work by building a
standard from example inputs in order to make data-driven prognostication or decisions expressed as outputs, more or
less than following factually stagnant program instructions.


\bigskip

Machine learning is meticulously related to and usually overhang with computational statistics; a branch of statistics
which also focuses in foreseeing through the use of computers. It has strong connection to mathematical optimization,
which delivers mechanism, concept and operation domains to the field. Machine learning procedures are deployed in a
variety of calculation tasks and problems where designing, developing and programming explicit procedures is not
possible. Some of the major applications of machine learning algorithms includes computer vision (object
identification), spam filtering (text classification), optical character recognition (OCR), search engines (search
Engine Optimization) and lots of other applications in near future. Machine learning is often amalgamated with data
mining, where the latter is a branch targets more on preparatory data analysis and is known as unsupervised learning.


\bigskip

Within the field of data analytics, machine learning is a way that is used to develop complicated standards and
procedure that lend themselves to foretelling the output. These analytical standards allows different communities of
researchers, data scientists, engineers, and analysts to {\textquotedbl}produce dependable, recurring decisions and
outputs{\textquotedbl} and uncover {\textquotedbl}hidden details {\textquotedbl} through learning from chronicled
relationships and trends in the data.


\bigskip

A more vague terminology for machine learning by Tom M. Mitchell is stated as ``\textit{For some performance value P and
T being the value for some class of task, any machine is said to learn from experience E, if for varying experience
value E, its performance at task T improves with E for measurement parameter P''}. This statement is quite important
for explaining machine learning in fundamentally operational rather than cognitive terms, thus this was followed by
Alan Turing's idea in his paper ``Computing Machinery and Intelligence'' that the argument ``Can machine think?'' be
exchanged by the argument ``Can machines do what we can do''.

In this project report we will be majorly discussing three classification algorithms for categorizing various songs
according to genre and then in the latter half of the report we will be using and implementing Lyrical analysis for
classification using bag of words strategy.


\bigskip

\subsection[Classification Algorithm]{Classification Algorithm}
\hypertarget{Toc451594956}{}\subsubsection[Naïve Bayes Classifier]{Naïve Bayes Classifier}
\hypertarget{Toc451594957}{}Naïve Bayes classifier is one of the common classifiers in machine learning that is based on
Bayes theorem under the strong assumption that the features involved in the classification process are totally
independent to each other's occurrence.

Naive Bayes has been under thorough study since the middle of 20\textsuperscript{th} century. It was introduced under a
distinct name into the text retrieval community in the latter half of 1950s, and stay a popular (basic standard)
\ procedure for text classification, the problem of deciding documents as belonging to one category or the other (such
as spam or important, sports ,fashion, politics, etc.) with word count as the features. With suitable pre-processing,
it is emulous in this territory with more advanced procedures including SVMs (Support Vector Machines). The Naïve Bayes
classifier also finds application in automatic medical diagnosis that is being used widely at various popular research
Institutes.


\bigskip

Following are some important points in favour of working with Naïve Bayes classifier: 

\liststyleWWNumxi
\begin{itemize}
\item Naive Bayes classifiers are easy to implement.
\item \ They are scalable to large extend, needing a number of parameters linear in the number of variables
(features/predictors) in a learning problem and there by solving problems rather very fast as compared to other
algorithms. 
\item Maximum-likelihood training with Naïve Bayes Algorithm can be done by calculating a closed-form expression (which
is a mathematical expression that can be evaluated in a finite number of operations) that takes asymptotically linear
time, instead of computationally expensive monotonous nearness as used for many other types of classifiers.
\end{itemize}

\bigskip

Naive Bayes procedures are known by many names, including simple Bayes and independence Bayes in various statistics and
computer science literature. All these names give hint the regarding the use of Bayes' theorem in the classifier's
decision rule, but naive Bayes is not typically a Bayesian method.

{\centering 
\includegraphics[width=8.308cm,height=1.138cm]{MusicGenreCategorization-img/MusicGenreCategorization-img008.png} \par}
\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{Using the naive independence assumption that}

{\centering 
\includegraphics[width=8.969cm,height=0.503cm]{MusicGenreCategorization-img/MusicGenreCategorization-img009.png} \par}
\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{For all}
\includegraphics[width=0.159cm,height=0.344cm]{MusicGenreCategorization-img/MusicGenreCategorization-img010.png}
\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{, this relationship is simplified to}

{\centering 
\includegraphics[width=8.07cm,height=1.164cm]{MusicGenreCategorization-img/MusicGenreCategorization-img011.png} \par}
\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{Since}\textstyleappleconvertedspace{\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{~}}
\includegraphics[width=2.725cm,height=0.476cm]{MusicGenreCategorization-img/MusicGenreCategorization-img012.png}
\textstyleappleconvertedspace{\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{~}}\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{is
constant given the input, we can use the following classification rule:}

{\centering 
\includegraphics[width=7.594cm,height=3.678cm]{MusicGenreCategorization-img/MusicGenreCategorization-img013.png} \par}
\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{And we can use Maximum A Posteriori (MAP) estimation to
estimate}\textstyleappleconvertedspace{\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{~}}
\includegraphics[width=0.979cm,height=0.476cm]{MusicGenreCategorization-img/MusicGenreCategorization-img014.png}
\textstyleappleconvertedspace{\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{~}}\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{and}
\includegraphics[width=1.773cm,height=0.503cm]{MusicGenreCategorization-img/MusicGenreCategorization-img015.png}
\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{; the former is then the relative frequency of
class}\textstyleappleconvertedspace{\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{~}}
\includegraphics[width=0.238cm,height=0.318cm]{MusicGenreCategorization-img/MusicGenreCategorization-img016.png}
\textstyleappleconvertedspace{\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{~}}\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{in
the training set.}

\textcolor[rgb]{0.11372549,0.12156863,0.13333334}{The different types of Naive Bayes classifiers differ mainly by the
assumptions they make regarding the distribution of conditional probabilities hence, there accuracy can differ
drastically depending on the nature of problem at hand. }


\bigskip

Now, we have the simple implementation of Naïve Bayes Classifier in sklearn 


\bigskip

 \includegraphics[width=15.214cm,height=4.154cm]{MusicGenreCategorization-img/MusicGenreCategorization-img017.png} 

In the above code,

Line Number 

\liststyleWWNumxii
\begin{enumerate}
\item Import the dataset module from sklearn library.
\item Loads the iris dataset from the module and saves the data in numpy array iris.
\item Imports the Gaussian Naïve Bayes classifier from sklearn.naive\_bayes.
\item Defines the NBclassifier.
\item Fit the data and makes prediction over it , saves the result into variable y\_pred
\item Prints the Number of mislabeled points out of total points.
\end{enumerate}

\bigskip

{\color{black}
\textbf{Hyper parameter tuning in Naïve Bayes Classifier }}

Now as we can see from the line number 4, there are no hyper parameters to be tuned to increase accuracy of the
prediction hence, we don't have to apply cross validation schemes like GridSearchCV to find tuning parameters for the
above algorithm. And for the same reason the algorithm generally has significantly lesser accuracy but is very easy to
implement and is fast to run over a large Dataset like Million Song Dataset.


\bigskip

{\color{black}
\textbf{Actual Code Implementation}}

\liststyleWWNumxiv
\begin{enumerate}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#~importing~necessary~libraries}\textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~numpy~as~np~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~math~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.cross\_validation~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~train\_test\_split~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.metrics~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~accuracy\_score~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.grid\_search~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~GridSearchCV~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.metrics~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~confusion\_matrix~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.naive\_bayes~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~GaussianNB~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~pickle~as~pkl~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~matplotlib.pyplot~as~plt~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#Loading~labels~and~features~from~the~pickle~file}\textcolor{black}{~~}
\item
\textcolor{black}{f=open(}\textcolor{blue}{{}'features'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~~}
\item \textcolor{black}{labels=pkl.load(f)~~}
\item \textcolor{black}{features=pkl.load(f)~~}
\item \textcolor{black}{f.close()~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~type(labels),type(features)~~~~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#spliting~data~into~training~and~testing~data}\textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}splitting~the~data~into~training~and~testing~set{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{feature\_train,feature\_test,label\_train,label\_test~=~train\_test\_split(features,~labels,~test\_size=0.20,~random\_state=42)}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Defining~the~classifier{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#Gaussian~naive\_bayes}\textcolor{black}{~~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.naive\_bayes~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~GaussianNB~~}
\item \textcolor{black}{clf~=~OneVsRestClassifier(GaussianNB())~~}
\item \textcolor{black}{x=[]~~}
\item \textcolor{black}{y=[]~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~len(feature\_train)~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~len(label\_train)~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~a~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~range(1,50):~~}
\item
\textcolor{black}{~~~pred~=~clf.fit(feature\_train[:len(feature\_train)/a],label\_train[:len(feature\_train)/a]).predict(feature\_test)~~}
\item \textcolor{black}{~~~x.append(len(feature\_train)/a)~~}
\item \textcolor{black}{~~~y.append(accuracy\_score(label\_test,pred))~~}
\item \textcolor{black}{plt.plot(x,y)~~}
\item \textcolor{black}{plt.title(}\textcolor{blue}{{}'Gaussian~Naive~Bayes~classifier'}\textcolor{black}{)~~}
\item \textcolor{black}{plt.xlabel(}\textcolor{blue}{{}'No.~of~Training~Samples'}\textcolor{black}{)~~}
\item \textcolor{black}{plt.ylabel(}\textcolor{blue}{{}'Accuracy~Score'}\textcolor{black}{)~~}
\item \textcolor{black}{plt.show()~~}
\end{enumerate}

\bigskip

{\color{black}
\textbf{Executing the Algorithm}}

After running the algorithm over different sizes of training and testing data set we found out following trends which
are plotted in the graph below with help of matplotlib.

 \includegraphics[width=16.51cm,height=8.943cm]{MusicGenreCategorization-img/MusicGenreCategorization-img018.png} 

\captionof{figure}{\hypertarget{Toc451594972}{}Gaussian Naive Bayes Classifier}

\bigskip

{\color{black}
\textbf{Observation }}

It is clear from the graph that after certain size of training data the algorithm breaks means the F1-score (accuracy)
gradually decreases with increase in training size. Generally it is the case that the accuracy increases with the
increase in the size of training set, but this case clearly suffers from the drawback of the Naïve Bayes classifier,
that is the features in the training data have high co-relation that is causing the algorithm to break.


\bigskip

{\color{black}
\textbf{Further Analysis}}

We can further analyse the results we got from the graph above. We generated the confusion matrix that is obtained.


\bigskip

\begin{center}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{4.1850004cm}|m{1.871cm}|m{1.2249999cm}|m{1.798cm}|m{1.574cm}|}
\hline
\centering Genre name &
\raggedleft precision &
\raggedleft recall &
\raggedleft F1-score &
\raggedleft\arraybslash support\\\hline
\centering classical pop and rock &
\raggedleft 0.26 &
\raggedleft 0.25 &
\raggedleft 0.26 &
\raggedleft\arraybslash 389\\\hline
\centering classical &
\raggedleft 0.59 &
\raggedleft 0.78 &
\raggedleft 0.67 &
\raggedleft\arraybslash 355\\\hline
\centering dance and electronica &
\raggedleft 0.39 &
\raggedleft 0.35 &
\raggedleft 0.37 &
\raggedleft\arraybslash 427\\\hline
\centering folk &
\raggedleft 0.44 &
\raggedleft 0.28 &
\raggedleft 0.34 &
\raggedleft\arraybslash 426\\\hline
\centering jazz and blues &
\raggedleft 0.51 &
\raggedleft 0.27 &
\raggedleft 0.36 &
\raggedleft\arraybslash 386\\\hline
\centering metal &
\raggedleft 0.46 &
\raggedleft 0.84 &
\raggedleft 0.60 &
\raggedleft\arraybslash 404\\\hline
\centering pop &
\raggedleft 0.29 &
\raggedleft 0.45 &
\raggedleft 0.36 &
\raggedleft\arraybslash 395\\\hline
\centering punk &
\raggedleft 0.25 &
\raggedleft 0.09 &
\raggedleft 0.13 &
\raggedleft\arraybslash 414\\\hline
\centering soul and reggae &
\raggedleft 0.42 &
\raggedleft 0.43 &
\raggedleft 0.43 &
\raggedleft\arraybslash 381\\\hline
\end{supertabular}
\end{center}

\bigskip


\bigskip

{\color{black}
\textbf{Results:}}

The highest obtained F1-score for the Naïve Bayes Classifier is 0.43 or 43\% at most at 750 training samples. Then, it
starts to decrease with increase with the increase of the size of training set .In other words the algorithm broke
after that point.


\bigskip


\bigskip

\subsubsection[Decision Tree Classifier]{Decision Tree Classifier}
\hypertarget{Toc451594958}{}Decision Tree classifier, sometimes called as regression trees, capitalizes a decision tree
as a predictive standard which drawing out observations about a feature to find about the item's labelled value.
Decision tree Classifier (Tree based method) is one of the predictive modelling techniques used in tree models where
the label variable can take a finite set of values and are known as classification trees. In Decision trees, what we
really care about is maximizing the information gain, when we split into branches according to certain features.

Few things to keep in mind while using this algorithm is concept of entropy or purity of a branch. As we already
discussed about maximizing information gain, so while building the tree we look for the features that can do the same,
i.e. maximize our information gain. 

 \includegraphics[width=16.51cm,height=4.066cm]{MusicGenreCategorization-img/MusicGenreCategorization-img019.png} 


\bigskip

Now, we can see simple implementation of Decision Tree Classifier in sklearn 

 \includegraphics[width=15.817cm,height=4.842cm]{MusicGenreCategorization-img/MusicGenreCategorization-img020.png} 


\bigskip

In the above code,

Line Number 

\liststyleWWNumxiii
\begin{enumerate}
\item Import the dataset module from sklearn library.
\item Imports cross validation score module from sklearn.cross\_validation.
\item Imports the Decision Tree classifier from sklearn.tree.
\item Defines the DecisionTreeClassifier with random state=0, which is a seed for random number generator.
\item Loads the iris dataset.
\item Using training and test data set divided into 10 equal folds cross validation score is being calculated and can be
seen in the next line.
\end{enumerate}

\bigskip

{\color{black}
\textbf{Optimizing and tuning parameters of decision Tree}}

First of all, we define a dictionary (pair of key and its value) of features and their possible values and this passed
as an argument to the K-fold cross validation scheme, itself computes the result for all possible combination of values
of features within their respective given ranges of value in constraints specified.

Exhaustive search using K-fold cross validation and GridSearchCV, what it does is split the original dataset into k
equal and separate folds and then randomly selects one fold and splits than into training and test data than generate
cross validation score that is the accuracy of prediction in that fold the same process is repeated over k-1 times and
all the cross validated score are taken and we take their mean.

This process is repeated over the all the ordered pairs of parameters and best values are reported back by the cross
validation procedure.

But as we can see that this is very tedious task both computationally and memory wise, so to overcome this problem we
used RandomSearchCV.

RandomSearchCV reduces the complexity by avoid exhaustive search and focusing certain chosen values within the parameter
grid.


\bigskip

{\color{black}
\textbf{Actual code Implementation}}

\liststyleWWNumxv
\begin{enumerate}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#~importing~neccessary~libraries}\textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~numpy~as~np~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~math~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.cross\_validation~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~train\_test\_split~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~tree~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.metrics~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~classification\_report~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~pickle~as~pkl~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~matplotlib.pyplot~as~plt~~}
\item \textcolor{black}{~~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#Loading~labels~and~features~from~the~pickle~file}\textcolor{black}{~~}
\item
\textcolor{black}{f=open(}\textcolor{blue}{{}'features'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~~}
\item \textcolor{black}{labels=pkl.load(f)~~}
\item \textcolor{black}{features=pkl.load(f)~~}
\item \textcolor{black}{f.close()~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~type(labels),type(features)~~~~}
\item 
\bigskip
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#spliting~data~into~training~and~testing~data}\textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}spliting~the~data~into~training~and~testing~set{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~~~}
\item
\textcolor{black}{feature\_train,feature\_test,label\_train,label\_test~=~train\_test\_split(features,~labels,~test\_size=0.20,~random\_state=42)}
\item \textcolor{black}{~~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Defining~the~classifier{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#Decision~Tree}\textcolor{black}{~~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{x=[]~~}
\item \textcolor{black}{y=[]~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~min\_split~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~range(1,400,5):~~}
\item \textcolor{black}{~~~~clf~=~tree.DecisionTreeClassifier(min\_samples\_split=min\_split)~~}
\item \textcolor{black}{~~~~clf.fit(feature\_train,label\_train)~~}
\item \textcolor{black}{~~~~pred=clf.predict(feature\_test)~~}
\item \textcolor{black}{~~~~accuracy=accuracy\_score(label\_test,pred)~~}
\item \textcolor{black}{~~~~x.append(min\_split)~~}
\item \textcolor{black}{~~~~y.append(accuracy)~~}
\item \textcolor{black}{plt.plot(x,y)~~}
\item
\textcolor{black}{plt.title(}\textcolor{blue}{{}'Min\_Samples\_Split~~Parameter~Optimization'}\textcolor{black}{)~~}
\item
\textcolor{black}{plt.xlabel(}\textcolor{blue}{{}'Value~of~min\_samples\_split~for~DecisionTreeClassifier~'}\textcolor{black}{)~~}
\item \textcolor{black}{plt.ylabel(}\textcolor{blue}{{}'Cross~Validated~Accuracy'}\textcolor{black}{)~~}
\item \textcolor{black}{plt.show()~~}
\end{enumerate}

\bigskip

{\color{black}
\textbf{Alternate method of applying Decision Tree Algorithm}}

\liststyleWWNumxvi
\begin{enumerate}
\item \textcolor{black}{clf~=~tree.DecisionTreeClassifier()~~}
\item \textcolor{black}{k\_range=range(1,5)~~}
\item \textcolor{black}{param\_grid=dict(min\_samples\_split=k\_range)~~}
\item
\textcolor{black}{grid=GridSearchCV(clf,param\_grid,cv=10,scoring=}\textcolor{blue}{{}'accuracy'}\textcolor{black}{)~~}
\item \textcolor{black}{grid.fit(features,labels)~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~grid.grid\_scores\_~~}
\item \textcolor{black}{~~}
\item
\textcolor{black}{grid\_mean\_scores=[result.mean\_validation\_score~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~result~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~grid.grid\_scores\_]~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~grid\_mean\_scores~~}
\item \textcolor{black}{plt.plot(k\_range,grid\_mean\_scores)~~}
\item \textcolor{black}{plt.title(}\textcolor{blue}{{}'Min\_Samples\_Split~Optimization'}\textcolor{black}{)~~}
\item \textcolor{black}{plt.xlabel(}\textcolor{blue}{{}'value~of~k~for~KNN'}\textcolor{black}{)~~}
\item \textcolor{black}{plt.ylabel(}\textcolor{blue}{{}'cross~validated~accuracy'}\textcolor{black}{)~~}
\item \textcolor{black}{plt.show()~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~grid.best\_score\_~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~grid.best\_params\_~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~grid.best\_estimator\_~~}
\end{enumerate}

\bigskip

{\color{black}
\textbf{Executing the Algorithm}}

So, for our purposes we tuned minimum sample split parameter of decision tree and obtained the following result and is
shown below using graph generated by python library matplotlib. The Algorithm took significant time to execute over the
dataset as the size of the dataset grew. As you can see in code rather than using GridSearchCV we have simply looped
over the values for minimum sample split parameter by simple human intuition that its value can vary between 2 to 500.
Minimum sample split Values above this range have underperformed drastically there for we took such constraint. 



\begin{figure}
\centering
\includegraphics[width=18.038cm,height=9.631cm]{MusicGenreCategorization-img/MusicGenreCategorization-img021.png}
\end{figure}
\captionof{figure}{\hypertarget{Toc451594973}{}Decision Tree Classifier}
As, it is clear from the graph the maximum accuracy obtained for classifying the genre was 0.465. It should be note that
here we not actually talking about accuracy specifically but describing it in terms of F1-score. As in the case of
multiclass classification it can be the case that the dataset be unbalanced, means to say that there can be very few
songs of certain genre and a lot of songs of any other category this gives an unseen advantage to these classes with
higher numbers and the algorithms favours those classes and works better predicting these classes.


\bigskip

This problem of biasing classes can be solved by two ways either changing class weights according to class frequency in
the dataset or by taking only equal number of samples from all the available classes. For our case we took the liberty
of using the second scenario as it would reduce both computational time as we were running the code over our local
system with strict system constraints as well as the algorithms performed better on balanced dataset.


\bigskip

{\color{black}
\textbf{Observation}}

It is clear from the graph that accuracy score (F1-score) initially with the increase of minimum\_sample\_split
parameter value up to 100, then after it starts to degrade the accuracy score of the algorithm .Optimal value obtained
somewhere near \ minimum\_sample\_split =105. 


\bigskip

{\color{black}
\textbf{Further Analysis }}

For Detailed recall and precision score of various genre categories we can see the confusion matrix for the above
algorithm at the optimal value of minimum\_sample\_split. 

\begin{center}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{3.9420002cm}|m{1.7579999cm}|m{1.148cm}|m{1.686cm}|m{1.477cm}|}
\hline
\centering Genre name &
\raggedleft precision &
\raggedleft recall &
\raggedleft F1-score &
\raggedleft\arraybslash support\\\hline
\centering classical pop and rock &
\raggedleft 0.28 &
\raggedleft 0.26 &
\raggedleft 0.27 &
\raggedleft\arraybslash 389\\\hline
\centering classical &
\raggedleft 0.68 &
\raggedleft 0.72 &
\raggedleft 0.70 &
\raggedleft\arraybslash 355\\\hline
\centering dance and electronica &
\raggedleft 0.47 &
\raggedleft 0.41 &
\raggedleft 0.44 &
\raggedleft\arraybslash 427\\\hline
\centering folk &
\raggedleft 0.40 &
\raggedleft 0.43 &
\raggedleft 0.41 &
\raggedleft\arraybslash 426\\\hline
\centering jazz and blues &
\raggedleft 0.39 &
\raggedleft 0.37 &
\raggedleft 0.38 &
\raggedleft\arraybslash 386\\\hline
\centering Metal &
\raggedleft 0.72 &
\raggedleft 0.73 &
\raggedleft 0.73 &
\raggedleft\arraybslash 404\\\hline
\centering pop &
\raggedleft 0.33 &
\raggedleft 0.39 &
\raggedleft 0.36 &
\raggedleft\arraybslash 395\\\hline
\centering punk &
\raggedleft 0.55 &
\raggedleft 0.48 &
\raggedleft 0.51 &
\raggedleft\arraybslash 414\\\hline
\centering soul and reggae &
\raggedleft 0.37 &
\raggedleft 0.40 &
\raggedleft 0.38 &
\raggedleft\arraybslash 381\\\hline
\centering avg / total &
\raggedleft 0.47 &
\raggedleft 0.46 &
\raggedleft 0.46 &
\raggedleft\arraybslash 3577\\\hline
\end{supertabular}
\end{center}

\bigskip

Accuracy: 0.464355605256

{\color{black}
\textbf{Results}}

The highest obtained F1-score for the Decision Tree Classifier is 0.465 or 46\% after tuning hyper parameters. Then, it
starts to decrease with increase with value of min\_sample\_split.


\bigskip


\bigskip

\subsubsection[K{}-nearest Neighbors Classifier]{K-nearest Neighbors Classifier}
\hypertarget{Toc451594959}{}It is one of the coolest algorithm for classification and regression problems in the sense
that it can accommodate to variety of problems of either type as compared to any other algorithm. Here, the value of k
indicates the number of nearest k neighbors to consider to classify the given data point in a category. To every
perfect thing there is always something bad in background same is the case with KNN algorithm though it is versatile in
application but it is computationally very expensive.


\bigskip

In this classification the outcome is class label. An object is decided to categorize to a category by a majority vote
of neighboring data points. If the value of K=1, then the given data point is categorized to single nearest neighbor.


\bigskip

This is a type of instance based learning algorithm a.k.a. lazy learning where the prediction is calculated according to
local distribution of data points and till then the pre{}-processing and calculation is delayed until it classifies
them.


\bigskip

Both for classification and regression problem the algorithm can be useful for assigning weights to the neighbors, so
the nearer neighbors contribute more to the expectation than the distant ones. 


\bigskip

The one of the major drawback that the algorithm sensitive to local structure of the data. 


\bigskip

 \includegraphics[width=14.03cm,height=9.208cm]{MusicGenreCategorization-img/MusicGenreCategorization-img022.png} 

\captionof{figure}[K Nearest Neighbor Example]{\hypertarget{Toc451594974}{}K Nearest Neighbor Example}

\bigskip

Now, here example of K-NN implementation of K-nearest Neighbor Algorithm in Sklearn

 \includegraphics[width=15.319cm,height=4.286cm]{MusicGenreCategorization-img/MusicGenreCategorization-img023.png} 


\bigskip

In the above code,

Line Number 

\liststyleWWNumxvii
\begin{enumerate}
\item Defines X as features numpy array.
\item Defines Y as a numpy array of labels.
\item Imports the KNeighborsClassifier from sklearn.neighbors.
\item Defines classifier neigh with nearest neighbor parameter set to 3.
\item Fits the classifier over the data.
\item Prints the prediction value of neighbor with X value 1.1. 
\item Prints the probability estimates for test data 0.9. 
\end{enumerate}

\bigskip

{\color{black}
\textbf{Optimizing or tuning the parameter of K-Nearest Neighbor Classifier}}

The parameter under consideration of tuning this time is n\_neighbors again we can apply exhaustive GridSearchCV or
simple iterative approach over the values of n\_neighbors can do the job for us. But since the K-nearest Neighbor is
computationally expensive we would prefer to have RandomizedSearchCV for tuning this parameter. 


\bigskip


\bigskip

{\color{black}
\textbf{Actual Code Implementation}}

\liststyleWWNumxviii
\begin{enumerate}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.cross\_validation~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~train\_test\_split~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~time~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.grid\_search~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~GridSearchCV~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.neighbors~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~KNeighborsClassifier~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~pickle~as~pkl~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~matplotlib.pyplot~as~plt~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#Loading~labels~and~features~from~the~pickle~file}\textcolor{black}{~~}
\item
\textcolor{black}{f=open(}\textcolor{blue}{{}'features'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~~}
\item \textcolor{black}{labels=pkl.load(f)~~}
\item \textcolor{black}{features=pkl.load(f)~~}
\item \textcolor{black}{f.close()~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~type(labels),type(features)~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#spliting~data~into~training~and~testing~data}\textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}spliting~the~data~into~training~and~testing~set{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{feature\_train,feature\_test,label\_train,label\_test~=~train\_test\_split(features,~labels,~test\_size=0.20,~random\_state=42)~~}
\item 
\bigskip
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Defining~the~classifier{\textquotedbl}}\textcolor{black}{~~}
\item 
\bigskip
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#~K~Nearest~Neighbor~Classifier}\textcolor{black}{~~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item 
\bigskip
\item \textcolor{black}{x=[]~~}
\item \textcolor{black}{y=[]~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~k~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~range(1,400,5):~~}
\item \textcolor{black}{~~~~clf~=~KNeighborsClassifier(n\_neighbors=k)~~}
\item \textcolor{black}{~~~~clf.fit(feature\_train,label\_train)~~}
\item \textcolor{black}{~~~~pred=clf.predict(feature\_test)~~}
\item \textcolor{black}{~~~~accuracy=accuracy\_score(label\_test,pred)~~~~~~}
\item \textcolor{black}{~~~~x.append(k)~~}
\item \textcolor{black}{~~~~y.append(accuracy)~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}k
:{\textquotedbl}}\textcolor{black}{,k~~}
\item \textcolor{black}{plt.plot(x,y)~~}
\item \textcolor{black}{plt.title(}\textcolor{blue}{{}'KNeighnors~Classifier~Optimization'}\textcolor{black}{)~~}
\item \textcolor{black}{plt.xlabel(}\textcolor{blue}{{}'Value~of~k~for~KNN'}\textcolor{black}{)~~}
\item \textcolor{black}{plt.ylabel(}\textcolor{blue}{{}'Accuracy~Score'}\textcolor{black}{)~~}
\item \textcolor{black}{plt.show()~~}
\end{enumerate}

\bigskip


\bigskip


\bigskip

{\color{black}
\textbf{Executing the Algorithm}}

So, for our purposes we tuned n\_neighbors parameter of K nearest Neighbors and obtained the following result and is
shown below using graph generated by python library matplotlib. The Algorithm took significant time to execute over the
dataset as the size of the dataset grew. As you can see in code rather than using GridSearchCV we have simply looped
over the values for n\_neighbor parameter by simple human intuition that its value can vary 1 to 400. n\_neighbor
values above this range have underperformed drastically there for we took such constraint.

\begin{figure}
\centering
\includegraphics[width=16.51cm,height=10.451cm]{MusicGenreCategorization-img/MusicGenreCategorization-img024.png}
\end{figure}

\bigskip

\captionof{figure}[K Nearest Neighbor Classifier]{\hypertarget{Toc451594975}{}K Nearest Neighbor Classifier}

\bigskip

As, it is clear from the graph the maximum accuracy obtained for classifying the genre was 0.519.

It should be note that here we not actually talking about accuracy specifically but describing it in terms of F1-score

{\color{black}
\textbf{Observation}}

It is clear from the graph that accuracy score (F1-score) initially with the increase of n\_neighbor parameter value up
to 15, then after it starts to degrade the accuracy score of the algorithm .Maximum value obtained near n\_neighbors
=15. 


\bigskip

{\color{black}
\textbf{Further Analysis }}

For Detailed recall and precision score of various genre categories we can see the confusion matrix for the above
algorithm at the optimal value of n\_neighbors. 

\begin{center}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{4.1850004cm}|m{1.871cm}|m{1.2249999cm}|m{1.798cm}|m{1.574cm}|}
\hline
\centering Genre name &
\raggedleft precision &
\raggedleft recall &
\raggedleft F1-score &
\raggedleft\arraybslash support\\\hline
\centering classical pop and rock &
\raggedleft 0.50 &
\raggedleft 0.80 &
\raggedleft 0.62 &
\raggedleft\arraybslash 4742\\\hline
\centering classical &
\raggedleft 0.64 &
\raggedleft 0.62 &
\raggedleft 0.63 &
\raggedleft\arraybslash 392\\\hline
\centering dance and electronica &
\raggedleft 0.59 &
\raggedleft 0.19 &
\raggedleft 0.29 &
\raggedleft\arraybslash 991\\\hline
\centering folk &
\raggedleft 0.52 &
\raggedleft 0.48 &
\raggedleft 0.50 &
\raggedleft\arraybslash 2666\\\hline
\centering jazz and blues &
\raggedleft 0.55 &
\raggedleft 0.17 &
\raggedleft 0.26 &
\raggedleft\arraybslash 856\\\hline
\centering metal &
\raggedleft 0.66 &
\raggedleft 0.47 &
\raggedleft 0.55 &
\raggedleft\arraybslash 409\\\hline
\centering pop &
\raggedleft 0.30 &
\raggedleft 0.02 &
\raggedleft 0.04 &
\raggedleft\arraybslash 422\\\hline
\centering punk &
\raggedleft 0.55 &
\raggedleft 0.22 &
\raggedleft 0.31 &
\raggedleft\arraybslash 644\\\hline
\centering soul and reggae &
\raggedleft 0.53 &
\raggedleft 0.17 &
\raggedleft 0.26 &
\raggedleft\arraybslash 798\\\hline
\centering avg / total &
\raggedleft 0.52 &
\raggedleft 0.52 &
\raggedleft 0.52 &
\raggedleft\arraybslash 11920\\\hline
\end{supertabular}
\end{center}

\bigskip

Accuracy: \ 0.515939597315


\bigskip

{\color{black}
\textbf{Results}}

The highest obtained F1-score for the Decision Tree Classifier is 0.519 or 52\% after tuning hyper parameters. Then, it
starts to decrease with increase with value of n\_neighbors.

\subsubsection[Random Forest Classifier (RFC)]{Random Forest Classifier (RFC)}
\hypertarget{Toc451594960}{}It is a meta-estimator that formulates and calculates a large number of decision trees
classifiers on various sub parts of the same data set and then averaging all those computed results to improve the
prediction accuracy and controlling over fitting the given data set.

This classifier is an ensemble learning procedure for classification that operates on multiple decision trees. They act
as saviours for decision trees preventing them from over fitting the dataset. The algorithm was developed by Leo
Breiman and A. Cutler along with some features later on added independently by Ho and Amit and German to build them
with control variance.

In this methodology, all the generated small trees gives some group of ill conditioned (biased) classifier and each one
of them weighs certain features more than others and then we draw out final decision tree based on all those biased
trees so that over all accuracy goes up for all the classes and at the same time the over fitting problem is solved. 


\bigskip

 \includegraphics[width=16.193cm,height=8.731cm]{MusicGenreCategorization-img/MusicGenreCategorization-img025.png} 

\captionof{figure}[Random Forest Example]{\hypertarget{Toc451594976}{}Random Forest Example}

\bigskip

Some important parameter of RFC that need to be considered while fitting and making predicts are:

\liststyleWWNumxix
\begin{itemize}
\item n\_estimators: This the number of biased decision trees to be formed while fitting the data set.
\item criterion: This function determines the quality of split. It has two values ``gini'' for impurity measure and
``entropy'' for measure of information gain .By default its value is set to gini.
\item max\_features: This is the number of features to be considered while fitting the data. For our purpose we have
used sqrt (square root (n\_features, which is the number of features)) which is same as using auto.
\item min\_sample\_split: This determines the minimum number of data points for splitting criterion.
\item n\_jobs: To added parallel processing if supported by the system.
\end{itemize}

\bigskip

{\color{black}
\textbf{Optimizing or tuning the parameter of Random Forest Classifier}}

Since in the case of Random forest the number of parameter which we would like to tune or optimize are many and since it
is a Gaussian process what we can do is apply Bayesian Optimization to tune the parameters. If we had used
RandomisedSearchCV It would have taken more than a day to complete its execution that too is not guaranteed to complete
in that interval.


\bigskip

{\color{black}
\textbf{Actual Code Implementation}}

\liststyleWWNumxx
\begin{enumerate}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~numpy~as~np~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~math~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.multiclass~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~OneVsRestClassifier~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.cross\_validation~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~train\_test\_split~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.ensemble~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~RandomForestClassifier~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~pickle~as~pkl~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~matplotlib.pyplot~as~plt~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.grid\_search~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~RandomizedSearchCV~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#Loading~labels~and~features~from~the~pickle~file}\textcolor{black}{~~}
\item
\textcolor{black}{f=open(}\textcolor{blue}{{}'features'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~~}
\item \textcolor{black}{labels=pkl.load(f)~~}
\item \textcolor{black}{features=pkl.load(f)~~}
\item \textcolor{black}{f.close()~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~type(labels),type(features)~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#spliting~data~into~training~and~testing~data}\textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}spliting~the~data~into~training~and~testing~set{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{feature\_train,feature\_test,label\_train,label\_test~=~train\_test\_split(features,~labels,~test\_size=0.20,~random\_state=42)~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Defining~the~classifier{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{clf=OneVsRestClassifier(RandomForestClassifier(min\_samples\_split=2,min\_samples\_leaf=1,n\_estimators=300))~~}
\item \textcolor{black}{clf.fit(feature\_train,label\_train)~~}
\item \textcolor{black}{pred=clf.predict(feature\_test)~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}confusion\_matrix:{\textquotedbl}}\textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{(classification\_report(label\_test,pred))~~}
\end{enumerate}

\bigskip

{\color{black}
\textbf{Executing the Algorithm}}

First of all, we calculated the optimal parameter values that were obtained by running Bayesian Optimization over the
algorithm and after few hours and roughly 30 iterations we got the optimized values for our algorithm. As can see we
have used meta-estimator that is, OneVsRestClassifier that is used for multiclass classification problem but we already
know that Random forest already supports multiclass classification but it is the fact that the every time we run this
algorithm all those biased trees are different every time. Therefore, we followed the OneVsRest strategy that runs this
algorithm n times and predicts or fits one class versus rest of other classes.


\bigskip


\bigskip

{\color{black}
\textbf{Observation}}

When we ran the above code and generated the confusion matrix for the same. The result that we got was much better than
what was calculated by any other algorithm. It reported a F1-Score of 0.62 or 62\% Accuracy.


\bigskip

{\color{black}
\textbf{Further Analysis }}

For Detailed recall and precision score of various genre categories we can see the confusion matrix for the above
algorithm at the optimal value of various parameters (min\_samples\_split, n\_estimators, and max\_features). 

{\centering
After Bayesian Optimization
\par}

\begin{center}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{4.1850004cm}|m{1.871cm}|m{1.2249999cm}|m{1.798cm}|m{1.574cm}|}
\hline
\centering Genre name &
\raggedleft precision &
\raggedleft recall &
\raggedleft F1-score &
\raggedleft\arraybslash support\\\hline
\centering classical pop and rock &
\raggedleft 0.46 &
\raggedleft 0.41 &
\raggedleft 0.43 &
\raggedleft\arraybslash 387\\\hline
\centering classical &
\raggedleft 0.77 &
\raggedleft 0.84 &
\raggedleft 0.80 &
\raggedleft\arraybslash 362\\\hline
\centering dance and electronica &
\raggedleft 0.63 &
\raggedleft 0.62 &
\raggedleft 0.62 &
\raggedleft\arraybslash 422\\\hline
\centering folk &
\raggedleft 0.54 &
\raggedleft 0.60 &
\raggedleft 0.57 &
\raggedleft\arraybslash 403\\\hline
\centering jazz and blues &
\raggedleft 0.61 &
\raggedleft 0.57 &
\raggedleft 0.59 &
\raggedleft\arraybslash 390\\\hline
\centering metal &
\raggedleft 0.82 &
\raggedleft 0.81 &
\raggedleft 0.81 &
\raggedleft\arraybslash 431\\\hline
\centering pop &
\raggedleft 0.47 &
\raggedleft 0.54 &
\raggedleft 0.50 &
\raggedleft\arraybslash 390\\\hline
\centering punk &
\raggedleft 0.70 &
\raggedleft 0.59 &
\raggedleft 0.64 &
\raggedleft\arraybslash 414\\\hline
\centering soul and reggae &
\raggedleft 0.54 &
\raggedleft 0.57 &
\raggedleft 0.56 &
\raggedleft\arraybslash 384\\\hline
\centering avg / total &
\raggedleft 0.62 &
\raggedleft 0.62 &
\raggedleft 0.62 &
\raggedleft\arraybslash 3583\\\hline
\end{supertabular}
\end{center}

\bigskip

{\color{black}
\textbf{Results}}

The highest obtained F1-score for the Random Forest Classifier is 0.62 or 62\% after optimizing hyperparameters.

\subsection[Bag of words Classification Scheme (Tf{}-Idf Vectorizer)]{Bag of words Classification Scheme (Tf-Idf
Vectorizer)}
\hypertarget{Toc451594961}{}The bag -of-words (BOW) method is simplified way of storing information of words in
documents in terms of word count document wise. This scheme is used in NLP (Natural Language Processing), in this model
a text (any word or sentence or document) is stored in the form of bag (multisets) of its words, irrespective of the
grammar and even the word sequence but just keeping their frequency.


\bigskip

In recent days, BOW (Bag-of-word) have been started to be deployed for solving computer vision problems. It is commonly
used for document classification, where the frequency of each word in the document is important and it is used as a
feature in prediction making.


\bigskip

{\centering 
\includegraphics[width=12.197cm,height=10.657cm]{MusicGenreCategorization-img/MusicGenreCategorization-img026.jpg}
\par}
\captionof{figure}[Bag of Word Example]{\hypertarget{Toc451594977}{}Bag of Word Example}

\bigskip

In this project we have implement this scheme over the lyrics of the songs that are there in the dataset and analyzed
them for classification. For that we created a python script that ran over the dataset and downloaded the available
lyrics from lyrics.wikia.com .Now , as the lyrics of any song are copyrighted material many of them are not available
online for downloading them. Out of 60000 songs in the dataset we got lyrics of 24000 but again to balance the dataset
we worked over 7500 song lyrics.


\bigskip

The script required used of beautifulsoup and requests library for downloading and parsing out the lyrics out of the
html document. After downloading those lyrics they were preprocessed all the punctuations, UTF-8 symbols were removed.


\bigskip

After this step, we used NLTK (National Language Toolkit) Library to remove all the stopwords from the lyrics as they
provide no information about the genres. After that we stemmed each word to its root using SnowBallStemmer so that
words that almost mean the same but differ in prefixes get bagged in same bag and hence improving accuracy for
predicting genre. Following this step, we used Tf-Idf (Term frequency --Inverse Document Frequency) Vectorizer to form
bag-of- words. 


\bigskip

After that we used Decision Tree classifier for to work on the generated bag of words.


\bigskip

{\color{black}
\textbf{Actual Code Implementation }}

\liststyleWWNumxxi
\begin{enumerate}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.feature\_extraction.text~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~TfidfVectorizer~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~pickle~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.metrics~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~classification\_report~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.metrics~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~accuracy\_score~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.cross\_validation~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~train\_test\_split~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.feature\_extraction.text~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~CountVectorizer~~}
\item
\textcolor{black}{stem\_data=open(}\textcolor{blue}{{\textquotedbl}stemmed\_text{\textquotedbl}}\textcolor{black}{,}\textcolor{blue}{{\textquotedbl}r{\textquotedbl}}\textcolor{black}{)~~}
\item \textcolor{black}{data=pickle.load(stem\_data)~~}
\item \textcolor{black}{~~}
\item
\textcolor{black}{feature\_train,feature\_test,label\_train,label\_test~=~train\_test\_split(features,~labels,~test\_size=0.20,~random\_state=42)~~}
\item \textcolor{black}{vectorizer=TfidfVectorizer(sublinear\_tf=True,max\_df=0.5)~~}
\item \textcolor{black}{feature\_train=vectorizer.fit\_transform(feature\_train)~~}
\item \textcolor{black}{feature\_test=vectorizer.transform(feature\_test).toarray()~~}
\item \textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~tree~~}
\item \textcolor{black}{clf~=~tree.DecisionTreeClassifier(min\_samples\_split=23)~~}
\item \textcolor{black}{clf.fit(feature\_train,label\_train)~~}
\item \textcolor{black}{pred=clf.predict(feature\_test)~~}
\item \textcolor{black}{importances~=~clf.feature\_importances\_~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~numpy~as~np~~}
\item \textcolor{black}{indices~=~np.argsort(importances)[::-1]~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{}'Feature~Ranking:~'}\textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~i~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~range(10):~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}\{\}~feature~no.\{\}~(\{\}){\textquotedbl}}\textcolor{black}{.format(i+1,indices[i],importances[indices[i]])~~}
\item \textcolor{black}{accuracy=accuracy\_score(label\_test,pred)~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}accuracy={\textquotedbl}}\textcolor{black}{,accuracy~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}confusion\_matrix:{\textquotedbl}}\textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{(classification\_report(label\_test,pred))~~}
\end{enumerate}

\bigskip

{\color{black}
\textbf{Observation}}

When we ran the above code and generated the confusion matrix for the same. The result that we got was much better than
what was calculated by bag-of-word scheme used in the base paper. It reported a F1-Score of 0.51 or 51\% accuracy.


\bigskip

 \includegraphics[width=16.51cm,height=12.383cm]{MusicGenreCategorization-img/MusicGenreCategorization-img027.png} 

\captionof{figure}[Bag of Word Scheme Accuracy plot]{\hypertarget{Toc451594978}{}Bag of Word Scheme Accuracy plot}

\bigskip

{\color{black}
\textbf{Results}}

The highest obtained F1-score for the Bag of words scheme with decision tree Classifier is 0.51 or 51\% after optimizing
hyperparameters.

\clearpage\section[Conclusion of Experimentation]{\textbf{Conclusion}\textbf{ of Experimentation}}
\hypertarget{Toc451594962}{}After thorough analysis with various implementation of the problem following results were
concluded.

\liststyleWWNumxxvii
\begin{enumerate}
\item Naïve Bayes classifier was very easy to implement and had very less execution time ,but due to strong co-relation
between the various features the algorithm went south and unexpected results were obtained
\item Talking about accuracies that we obtained from the remaining three classifiers
\end{enumerate}
\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{7.438cm}|m{7.3830004cm}|}
\hline
\centering Classifier Name &
\centering\arraybslash F1-Score\\\hline
Naïve Bayes &
\centering\arraybslash 0.43\\\hline
Decision Trees &
\centering\arraybslash 0.46\\\hline
K- Nearest Neighbor &
\centering\arraybslash 0.52\\\hline
Random Forest &
\centering\arraybslash 0.62\\\hline
\end{supertabular}
\end{flushleft}

\bigskip


\bigskip

\liststyleWWNumxxvii
\setcounter{saveenum}{\value{enumi}}
\begin{enumerate}
\setcounter{enumi}{\value{saveenum}}
\item Bag of words approach went very well for the smaller dataset, its accuracy is sure to go up if larger number of
songs were used.
\item Unbalanced Datasets produce underperforming results.
\item Appropriate preprocessing of the dataset is required before it can be used in machine learning Algorithms.
\end{enumerate}

\bigskip


\bigskip


\bigskip


\bigskip

\clearpage\section[Important Codes In various phases of project]{\textbf{Important Codes In various phases of project}}
\hypertarget{Toc451594963}{}{\color{black}
\textbf{FeatureExtraction.py}}

\ \ This script helps in extracting out the various features and labels in the million song genre dataset and storing
them in suitable format so that they are available to apply machine learning algorithms over them

\liststyleWWNumxxii
\begin{enumerate}
\item \textcolor{blue}{{}'{}'}\textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{This~file~is~extracting~out~the~features~from~the~million~song~genre~dataset~as~per~our~requirement.}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{Making~even, balanced~dataset~to~work~upon.}\textcolor{black}{~}
\item \textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~~}
\item \textcolor{black}{~~}
\item \textcolor{blue}{{}'{}'}\textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~}
\item \textcolor{black}{~}\textcolor[rgb]{0.0,0.50980395,0.0}{No~of~songs:~~59600}\textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{**************************************************************************}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{Dataset~composition:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{jazz~and~blues~4334}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{classic~pop~and~rock~23895}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{classical~1874}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{punk~3200}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{metal~2103}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{pop~1617}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{dance~and~electronica~4935}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{hip-hop~434}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{soul~and~reggae~4016}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{folk~13192}\textcolor{black}{~}
\item \textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'}\textcolor{black}{~~}
\item \textcolor{black}{~~}
\item \textcolor{blue}{{}'{}'}\textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{No~of~songs:~~59600}\textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{**************************************************************************}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{Dataset~composition:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{jazz~and~blues~4334}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{classic~pop~and~rock~23895}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{classical~1874}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{punk~3200}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{metal~2103}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{pop~2051}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{dance~and~electronica~4935}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{soul~and~reggae~4016}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{folk~13192}\textcolor{black}{~}
\item \textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~~}
\item \textcolor{blue}{{}'{}'}\textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{No~of~songs:~~59600}\textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{**************************************************************************}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{Dataset~composition:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{jazz~and~blues:~~2001}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{classic~pop~and~rock:~~2001}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{classical:~~1874}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{punk:~~2001}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{metal:~~2001}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{pop:~~2001}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{dance~and~electronica:~~2001}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{soul~and~reggae:~~2001}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{folk:~~2001}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~~}
\item \textcolor{black}{~~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textcolor{blue}{{}'{}'}\textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{Helper~functions~for~pre-processing~the~data~before~creating~pickle~object}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{def}}\textcolor{black}{~processdata(lines):~~}
\item \textcolor{black}{~~~~features=[]~~}
\item \textcolor{black}{~~~~labels=[]~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~line~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~lines:~~}
\item \textcolor{black}{~~~~~~~~temp=line.split(}\textcolor{blue}{{}','}\textcolor{black}{)~~}
\item \textcolor{black}{~~~~~~~~labels.append(temp[0])~~}
\item \textcolor{black}{~~~~~~~~l=[]~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~feature~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~temp[4:]:~~}
\item \textcolor{black}{~~~~~~~~~~~~l.append(float(feature))~~}
\item \textcolor{black}{~~~~~~~~features.append(l)~~}
\item \textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{return}}\textcolor{black}{~labels,features~~}
\item \textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{def}}\textcolor{black}{~targetFeatureSplit(~data~):~~}
\item
\textcolor{black}{~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{{\textquotedbl}{\textquotedbl}{\textquotedbl}~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~given~a~numpy~array~like~the~one~returned~from}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~featureFormat,~separate~out~the~first~feature}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~and~put~it~into~its~own~list~(this~should~be~the~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~quantity~you~want~to~predict)}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~return~targets~and~features~as~separate~lists}\textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~(sklearn~can~generally~handle~both~lists~and~numpy~arrays~as~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~input~formats~when~training/predicting)}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~{\textquotedbl}{\textquotedbl}{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~~~~target~=~[]~~}
\item \textcolor{black}{~~~~features~=~[]~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~item~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~data:~~}
\item \textcolor{black}{~~~~~~~~target.append(~item[0]~)~~}
\item \textcolor{black}{~~~~~~~~features.append(~item[1:]~)~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{return}}\textcolor{black}{~target,~features~~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~numpy~as~np~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~os~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~pickle~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.preprocessing~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~MinMaxScaler~~}
\item
\textcolor{black}{b=open(}\textcolor{blue}{{\textquotedbl}balanced\_msd.txt{\textquotedbl}}\textcolor{black}{,}\textcolor{blue}{{}'w'}\textcolor{black}{)~~}
\item \textcolor{black}{line\_number=0~~}
\item \textcolor{black}{genres=\{\}~~}
\item
\textcolor{black}{with~open(}\textcolor{blue}{{}'msd\_genre\_dataset.txt'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~as~f:~~}
\item \textcolor{black}{~~~~lines=f.readlines()~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~line~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~lines:~~}
\item \textcolor{black}{~~~~~~~~line\_number+=1~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{~line\_number{\textgreater}10:~~}
\item \textcolor{black}{~~~~~~~~~~~~temp=line.split(}\textcolor{blue}{{}','}\textcolor{black}{)~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{(temp[0]==}\textcolor{blue}{{}'hip-hop'}\textcolor{black}{):~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~temp[0]=}\textcolor{blue}{{}'pop'}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~~~~~b.write(}\textcolor{blue}{{\textquotedbl},{\textquotedbl}}\textcolor{black}{.join(temp))~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{try}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~genres[temp[0]]+=1~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{except}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~genres[temp[0]]=1~~~~~}
\item \textcolor{blue}{{}'{}'}\textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{print~{\textquotedbl}File~composition{\textquotedbl}}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{for~g~in~genres.keys():}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~print~g+{\textquotedbl}:~{\textquotedbl},genres[g]}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~~}
\item \textcolor{black}{b.close()~~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#~phase~2:~balancing~the~dataset}\textcolor{black}{~~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textcolor{black}{genres=\{\}~~}
\item
\textcolor{black}{b=open(}\textcolor{blue}{{\textquotedbl}balanced\_msd\_final.txt{\textquotedbl}}\textcolor{black}{,}\textcolor{blue}{{}'w'}\textcolor{black}{)~~}
\item
\textcolor{black}{with~open(}\textcolor{blue}{{}'balanced\_msd.txt'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~as~f:~~}
\item \textcolor{black}{~~~~lines=f.readlines()~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~line~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~lines:~~}
\item \textcolor{black}{~~~~~~~~temp=line.split(}\textcolor{blue}{{}','}\textcolor{black}{)~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{try}}\textcolor{black}{:~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{(genres[temp[0]]{\textless}=2000):~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~genres[temp[0]]+=1~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~b.write(}\textcolor{blue}{{\textquotedbl},{\textquotedbl}}\textcolor{black}{.join(temp))~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{except}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~genres[temp[0]]=1~~}
\item
\textcolor{black}{~~~~~~~~~~~~b.write(}\textcolor{blue}{{\textquotedbl},{\textquotedbl}}\textcolor{black}{.join(temp))~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{try}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~os.remove(}\textcolor{blue}{{}'balanced\_msd.txt'}\textcolor{black}{)~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{except}}\textcolor{black}{:~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}balanced\_msd.txt~is~missing~{\textquotedbl}}\textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#{\textquotedbl}}\textcolor{black}{~~~~~~~~~~~~~~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}~~~~~~~~~~~~~~~~~~~~~~~~~~Dataset~composition{\textquotedbl}}\textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#{\textquotedbl}}\textcolor{black}{~~~~~~~~~~~~~~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~g~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~genres.keys():~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~g+}\textcolor{blue}{{\textquotedbl}:~{\textquotedbl}}\textcolor{black}{,genres[g]~~}
\item \textcolor{black}{b.close()~~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textcolor{blue}{{}'{}'}\textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{Formatting~the~data~into~numpy~arrays~,suitable~using~them~on~the~go.~}\textcolor{black}{~}
\item \textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{Using~pickle~to~save~the~serialised~object~for~future~analysis~of~dataset}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{by~various~algorithms}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item
\textcolor{black}{with~open(}\textcolor{blue}{{}'balanced\_msd\_final.txt'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~as~f:~~}
\item \textcolor{black}{~~~~lines=f.readlines()~~}
\item \textcolor{black}{~~~~labels,features=processdata(lines)~~}
\item \textcolor{black}{np\_features=np.array(features)~~}
\item \textcolor{black}{labels=np.array(labels)~~}
\item \textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#feature~Scaling~on~dataset~}\textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textcolor{black}{scaler=MinMaxScaler()~~}
\item \textcolor{black}{np\_features=scaler.fit\_transform(np\_features)~~}
\item 
\bigskip
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#pickling~the~data}\textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}\textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#print~len(features)}\textcolor{black}{~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#print~len(np\_features)}\textcolor{black}{~~}
\item \textcolor{black}{~~}
\item
\textcolor{black}{f=open(}\textcolor{blue}{{\textquotedbl}features{\textquotedbl}}\textcolor{black}{,}\textcolor{blue}{{\textquotedbl}w{\textquotedbl}}\textcolor{black}{)~~}
\item \textcolor{black}{pickle.dump(labels,f)~~}
\item \textcolor{black}{pickle.dump(np\_features,f)~~}
\item \textcolor{black}{f.close()~~}
\item \textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Successfully~did~preprocessing~without~any~error{\textquotedbl}}\textcolor{black}{~~}
\end{enumerate}

\bigskip

{\color{black}
\textbf{BayesianOptimization.py}}

This code helps in Tuning the hyper parameters of random forest using Bayesian Optimization library 

\liststyleWWNumxxiii
\begin{enumerate}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#from~\_\_future\_\_~import~print\_function}\textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~\_\_future\_\_~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~division~~}
\item \textcolor{black}{~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{rom}}\textcolor{black}{~sklearn.datasets~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~make\_classification~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.cross\_validation~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~cross\_val\_score~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.ensemble~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~RandomForestClassifier~as~RFC~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.cross\_validation~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~train\_test\_split~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.svm~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~SVC~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~pickle~as~pkl~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~bayes\_opt~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~BayesianOptimization~~}
\item \textcolor{black}{~}
\item
\textcolor{black}{f=open(}\textcolor{blue}{{}'features'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~~}
\item \textcolor{black}{labels=pkl.load(f)~~}
\item \textcolor{black}{features=pkl.load(f)~~}
\item \textcolor{black}{f.close()~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}spliting~the~data~into~training~and~testing~set{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{feature\_train,feature\_test,label\_train,label\_test~=~train\_test\_split(features,~labels,~test\_size=0.20,~random\_state=42)~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Defining~the~classifier{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{def}}\textcolor{black}{~rfccv(n\_estimators,~min\_samples\_split,~max\_features):~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{return}}\textcolor{black}{~cross\_val\_score(RFC(n\_estimators=int(n\_estimators),~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~min\_samples\_split=int(min\_samples\_split),~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~max\_features=min(max\_features,~0.999),~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~random\_state=2),~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~~~~feature\_train,~label\_train,~}\textcolor{blue}{{}'f1\_weighted'}\textcolor{black}{,~cv=5).mean()~~}
\item \textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{~\_\_name\_\_~==~}\textcolor{blue}{{\textquotedbl}\_\_main\_\_{\textquotedbl}}\textcolor{black}{:~~}
\item
\textcolor{black}{~~~~rfcBO~=~BayesianOptimization(rfccv,~\{}\textcolor{blue}{{}'n\_estimators'}\textcolor{black}{:~(250,~400),~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\textcolor{blue}{{}'min\_samples\_split'}\textcolor{black}{:~(2,15),~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\textcolor{blue}{{}'max\_features'}\textcolor{black}{:~(0.1,~0.999)\})~~}
\item \textcolor{black}{~}
\item \textcolor{black}{~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{(}\textcolor{blue}{{}'-'}\textcolor{black}{*53)~~}
\item \textcolor{black}{~~~~rfcBO.maximize()~~}
\item \textcolor{black}{~}
\item
\textcolor{black}{~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{(}\textcolor{blue}{{}'-'}\textcolor{black}{*53)~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{(}\textcolor{blue}{{}'Final~Results'}\textcolor{black}{)~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{(}\textcolor{blue}{{}'RFC:~\%f'}\textcolor{black}{~\%~rfcBO.res[}\textcolor{blue}{{}'max'}\textcolor{black}{][}\textcolor{blue}{{}'max\_val'}\textcolor{black}{])~~}
\end{enumerate}

\bigskip

{\color{black}
\textbf{Lyrics.py}}

The Script helps in downloading and parsing the lyrics through lyrics.wikia.com and saving them into a text file for
further processing.

\liststyleWWNumxxiv
\begin{enumerate}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~requests~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~bs4~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~BeautifulSoup,~Comment,~NavigableString~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~sys,~codecs,~json~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{def}}\textcolor{black}{~getLyrics(singer,~song):~~}
\item \textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#Replace~spaces~with~\_}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~singer~=~singer.replace(}\textcolor{blue}{{}'~'}\textcolor{black}{,~}\textcolor{blue}{{}'\_'}\textcolor{black}{)~~}
\item
\textcolor{black}{~~~~~~~~song~=~song.replace(}\textcolor{blue}{{}'~'}\textcolor{black}{,~}\textcolor{blue}{{}'\_'}\textcolor{black}{)~~}
\item
\textcolor{black}{~~~~~~~~r~=~requests.get(}\textcolor{blue}{{}'http://lyrics.wikia.com/\{0\}:\{1\}'}\textcolor{black}{.format(singer,song))~~}
\item \textcolor{black}{~~~~~~~~s~=~BeautifulSoup(r.text,}\textcolor{blue}{{}'lxml'}\textcolor{black}{)~~}
\item \textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#Get~main~lyrics~holder}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~lyrics~=~s.find(}\textcolor{blue}{{\textquotedbl}div{\textquotedbl}}\textcolor{black}{,\{}\textcolor{blue}{{}'class'}\textcolor{black}{:}\textcolor{blue}{{}'lyricbox'}\textcolor{black}{\})~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{~lyrics~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{is}}\textcolor{black}{~None:~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#raise~ValueError({\textquotedbl}Song~or~Singer~does~not~exist~or~the~API~does~not~have~Lyrics{\textquotedbl})}\textcolor{black}{~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{return}}\textcolor{black}{~None~~}
\item \textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#Remove~Scripts}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~[s.extract()~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~s~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~lyrics(}\textcolor{blue}{{}'script'}\textcolor{black}{)]~~}
\item \textcolor{black}{~~~~~~~}
\item \textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#Remove~Comments}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~comments~=~lyrics.findAll(text=}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{lambda}}\textcolor{black}{~text:isinstance(text,~Comment))~~}
\item
\textcolor{black}{~~~~~~~~[comment.extract()~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~comment~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~comments]~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#Remove~unecessary~tags}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~tag~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~[}\textcolor{blue}{{}'div'}\textcolor{black}{,}\textcolor{blue}{{}'i'}\textcolor{black}{,}\textcolor{blue}{{}'b'}\textcolor{black}{,}\textcolor{blue}{{}'a'}\textcolor{black}{]:~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~match~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~lyrics.findAll(tag):~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~match.replaceWithChildren()~~}
\item
\textcolor{black}{~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#Get~output~as~a~string~and~remove~non~unicode~characters~and~replace~{\textless}br{\textgreater}~with~newlines}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~lyrics=~str(lyrics).replace(}\textcolor{blue}{{}'{\textbackslash}n'}\textcolor{black}{,}\textcolor{blue}{{}'{}'}\textcolor{black}{).replace(}\textcolor{blue}{{}'{\textless}br/{\textgreater}'}\textcolor{black}{,}\textcolor{blue}{{}'~'}\textcolor{black}{)~~}
\item \textcolor{black}{~~~~~~~~output=lyrics[22:-6:]~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{try}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{return}}\textcolor{black}{~output~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{except}}\textcolor{black}{:~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{return}}\textcolor{black}{~output.encode(}\textcolor{blue}{{}'utf-8'}\textcolor{black}{)~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{genres=\{\}~~}
\item \textcolor{black}{lyrics\_found=0~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~codecs~~}
\item
\textcolor{black}{y=codecs.open(}\textcolor{blue}{{\textquotedbl}songLyrics1.txt{\textquotedbl}}\textcolor{black}{,}\textcolor{blue}{{\textquotedbl}w{\textquotedbl}}\textcolor{black}{)~~}
\item \textcolor{black}{line\_num=0~~}
\item
\textcolor{black}{with~codecs.open(}\textcolor{blue}{{}'msd\_genre\_dataset.txt'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~as~f:~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~line~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~f:~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{~line\_num{\textless}=27432:~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{pass}}\textcolor{black}{~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{else}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~temp=line.split(}\textcolor{blue}{{}','}\textcolor{black}{)~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{\#print~{\textquotedbl}artist~name={\textquotedbl},temp[2],{\textquotedbl}Title={\textquotedbl},temp[3]}\textcolor{black}{~~}
\item \textcolor{black}{~~~~~~~~~~~~lyrics=getLyrics(temp[2],temp[3])~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{(lyrics!=None):~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{(lyrics.split(}\textcolor{blue}{{}'~'}\textcolor{black}{)[0]!=}\textcolor{blue}{{}'{\textless}span'}\textcolor{black}{):~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{try}}\textcolor{black}{:~~~~~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~genres[temp[0]]+=1~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{except}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~~~~~genres[temp[0]]=1~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~lyrics\_found+=1~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~y.write(temp[0]+}\textcolor{blue}{{}'**/**'}\textcolor{black}{+lyrics)~~}
\item \textcolor{black}{~~~~~~~~~~~~~~~~~~~~y.write(}\textcolor{blue}{{}'{\textbackslash}n'}\textcolor{black}{)~~}
\item \textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{(line\_num\%100==0):~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}At~present~progress{\textbackslash}n{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Songs~processed:~{\textquotedbl}}\textcolor{black}{+~str(line\_num)~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}Lyrics~saved~upto~now:{\textquotedbl}}\textcolor{black}{~+str(lyrics\_found)~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~gen~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~genres.keys():~~}
\item
\textcolor{black}{~~~~~~~~~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~gen,}\textcolor{blue}{{\textquotedbl}:{\textquotedbl}}\textcolor{black}{,genres[gen]~~}
\item \textcolor{black}{~~~~~~~~line\_num+=1~~}
\item \textcolor{black}{y.close()~~}
\end{enumerate}

\bigskip


\bigskip

{\color{black}
\textbf{ProcessLyrics.py}}

After the lyrics have been downloaded and saved into text file, it has to be preprocessed before it, bag of words is
generated and this script helps in the same.

\liststyleWWNumxxv
\begin{enumerate}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~\_\_future\_\_~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~division~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~nltk.corpus~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~stopwords~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~nltk.stem~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~SnowballStemmer~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~unicodedata~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~codecs~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~string~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~pickle~~}
\item \textcolor{black}{stemmer~=~SnowballStemmer(}\textcolor{blue}{{}'english'}\textcolor{black}{)~~}
\item
\textcolor{black}{cachedStopWords~=~stopwords.words(}\textcolor{blue}{{\textquotedbl}english{\textquotedbl}}\textcolor{black}{)~~}
\item \textcolor{black}{data=[]~~}
\item
\textcolor{black}{with~codecs.open(}\textcolor{blue}{{}'finalLyricsList2.txt'}\textcolor{black}{,}\textcolor{blue}{{}'r'}\textcolor{black}{)~as~f:~~}
\item \textcolor{black}{~~~~lines=f.readlines()~~}
\item \textcolor{black}{~~~~count=0~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~line~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~lines:~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~~~~~~~labels,features=line.split(}\textcolor{blue}{{}'**/**'}\textcolor{black}{)~~~~~~~}
\item
\textcolor{black}{~~~~~~~~features~=~unicode(features,~}\textcolor{blue}{{\textquotedbl}utf-8{\textquotedbl}}\textcolor{black}{)~~}
\item
\textcolor{black}{~~~~~~~~features~=~unicodedata.normalize(}\textcolor{blue}{{}'NFKD'}\textcolor{black}{,features).encode(}\textcolor{blue}{{}'ascii'}\textcolor{black}{,}\textcolor{blue}{{}'ignore'}\textcolor{black}{)~~}
\item \textcolor{black}{~~~~~~~~features=features.translate(None,~string.punctuation)~~}
\item \textcolor{black}{~~~~~~~~features=features.lower()~~~~}
\item
\textcolor{black}{~~~~~~~~features=[word~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~word~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~features.split()~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{~word~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{not}}\textcolor{black}{~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~cachedStopWords]~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~word~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~features:~~}
\item \textcolor{black}{~~~~~~~~~~~~stemmer.stem(word)~~}
\item
\textcolor{black}{~~~~~~~~features=}\textcolor{blue}{{\textquotedbl}~{\textquotedbl}}\textcolor{black}{.join(features)~~}
\item \textcolor{black}{~~~~~~~~data.append([labels,features])~~}
\item \textcolor{black}{~~~~~~~~count+=1~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{if}}\textcolor{black}{(count\%500==0):~~}
\item
\textcolor{black}{~~~~~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}completed:{\textquotedbl}}\textcolor{black}{,count/len(lines)*100~~}
\item \textcolor{black}{~~~~~~~~~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{~~~~genres=\{\}~~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~line~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~lines:~~}
\item \textcolor{black}{~~~~~~~~labels,features=line.split(}\textcolor{blue}{{}'**/**'}\textcolor{black}{)~~~~~~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{try}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~genres[labels]+=1~~}
\item \textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{except}}\textcolor{black}{:~~}
\item \textcolor{black}{~~~~~~~~~~~~genres[labels]=0~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}File~composition{\textquotedbl}}\textcolor{black}{~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~g~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~genres.keys():~~}
\item
\textcolor{black}{~~~~~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~g+}\textcolor{blue}{{\textquotedbl}:~{\textquotedbl}}\textcolor{black}{,genres[g]~~}
\item \textcolor{black}{~~~~~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#saving~into~serialized~object}\textcolor{black}{~~}
\item
\textcolor{black}{stem\_data=open(}\textcolor{blue}{{\textquotedbl}stemmed\_text{\textquotedbl}}\textcolor{black}{,}\textcolor{blue}{{}'wb'}\textcolor{black}{)~~}
\item \textcolor{black}{pickle.dump(data,stem\_data)~~}
\item \textcolor{black}{stem\_data.close()~~}
\end{enumerate}

\bigskip

{\color{black}
\textbf{load\_stem\_data.py}}

Script is working with processed data to use bag of words and make prediction

\liststyleWWNumxxvi
\begin{enumerate}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.feature\_extraction.text~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~TfidfVectorizer~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~pickle~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.metrics~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~classification\_report~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.metrics~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~accuracy\_score~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.cross\_validation~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~train\_test\_split~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn.feature\_extraction.text~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~CountVectorizer~~}
\item
\textcolor{black}{stem\_data=open(}\textcolor{blue}{{\textquotedbl}stemmed\_text{\textquotedbl}}\textcolor{black}{,}\textcolor{blue}{{\textquotedbl}r{\textquotedbl}}\textcolor{black}{)~~}
\item \textcolor{black}{data=pickle.load(stem\_data)~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#print~data[0]}\textcolor{black}{~~}
\item \textcolor{blue}{{}'{}'}\textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{b=open({\textquotedbl}finalLyricsList.txt{\textquotedbl},{\textquotedbl}w{\textquotedbl})}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{with~open('songLyrics.txt','r')~as~f:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~lines=f.readlines()}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~for~line~in~lines:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~labels,features=line.split('**/**')~~~~~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~if(labels=='hip-hop'):}\textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~~~~~b.write({\textquotedbl}pop**/**{\textquotedbl}+features)}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~if(labels!='classical'~and~labels!='hip-hop'):}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~~~~~b.write(line)}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{b.close()}\textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{b=open({\textquotedbl}finalLyricsList.txt{\textquotedbl},{\textquotedbl}r{\textquotedbl})}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{lines=b.readlines()}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{genres=\{\}~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{for~line~in~lines:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~labels,features=line.split('**/**')~~~~~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~try:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~genres[labels]+=1}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~except:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~genres[labels]=0}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{print~{\textquotedbl}File~composition{\textquotedbl}}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{for~g~in~genres.keys():}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~print~g+{\textquotedbl}:~{\textquotedbl},genres[g]}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~~~~~~~}
\item \textcolor{black}{~~}
\item \textcolor{blue}{{}'{}'}\textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~}
\item \textcolor{black}{~}\textcolor[rgb]{0.0,0.50980395,0.0}{genres=\{\}}\textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{b=open({\textquotedbl}finalLyricsList2.txt{\textquotedbl},{\textquotedbl}w{\textquotedbl})}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{with~open('finalLyricsList.txt','r')~as~f:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~lines=f.readlines()}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~for~line~in~lines:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~labels,features=line.split('**/**')~~~~~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~try:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~~~~~genres[labels]+=1}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~except:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~~~~~genres[labels]=0}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~if(genres[labels]{\textless}=1000):}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~~~~~b.write(line)}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{b.close()}\textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{b=open({\textquotedbl}finalLyricsList2.txt{\textquotedbl},{\textquotedbl}r{\textquotedbl})}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{lines=b.readlines()}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{genres=\{\}~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{for~line~in~lines:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~labels,features=line.split('**/**')~~~~~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~try:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~genres[labels]+=1}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~except:}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~genres[labels]=0}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{print~{\textquotedbl}File~composition{\textquotedbl}}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{for~g~in~genres.keys():}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~print~g+{\textquotedbl}:~{\textquotedbl},genres[g]}\textcolor{black}{~}
\item \textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{def}}\textcolor{black}{~targetFeatureSplit(~data~):~~}
\item
\textcolor{black}{~~~~}\textcolor[rgb]{0.0,0.50980395,0.0}{{\textquotedbl}{\textquotedbl}{\textquotedbl}~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~given~a~numpy~array~like~the~one~returned~from}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~featureFormat,~separate~out~the~first~feature}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~and~put~it~into~its~own~list~(this~should~be~the~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~quantity~you~want~to~predict)}\textcolor{black}{~}
\item \textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~return~targets~and~features~as~separate~lists}\textcolor{black}{~}
\item \textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~(sklearn~can~generally~handle~both~lists~and~numpy~arrays~as~}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~~~~~input~formats~when~training/predicting)}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{~~~~{\textquotedbl}{\textquotedbl}{\textquotedbl}}\textcolor{black}{~~}
\item \textcolor{black}{~~~~target~=~[]~~}
\item \textcolor{black}{~~~~features~=~[]~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~item~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~data:~~}
\item \textcolor{black}{~~~~~~~~target.append(~item[0]~)~~}
\item \textcolor{black}{~~~~~~~~features.append(~item[1]~)~~~}
\item \textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{return}}\textcolor{black}{~target,~features~~}
\item \textcolor{black}{~~}
\item \textcolor{black}{labels,features=targetFeatureSplit(data)~~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{\#print~data[0]}\textcolor{black}{~~}
\item \textcolor{blue}{{}'{}'}\textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~}
\item \textcolor{black}{~}
\item
\textcolor[rgb]{0.0,0.50980395,0.0}{feature\_train,feature\_test,label\_train,label\_test~=~train\_test\_split(features,~labels,~test\_size=0.20,~random\_state=42)}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{vectorizer=CountVectorizer()}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{train\_counts~=~vectorizer.fit\_transform(feature\_train)}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{print~train\_counts.shape}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{print~train\_counts}\textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{print~vectorizer.vocabulary\_.get(u'la')}\textcolor{black}{~}
\item \textcolor{black}{~}
\item \textcolor[rgb]{0.0,0.50980395,0.0}{{}'{}'{}'}\textcolor{black}{~~}
\item
\textcolor{black}{feature\_train,feature\_test,label\_train,label\_test~=~train\_test\_split(features,~labels,~test\_size=0.20,~random\_state=42)~~}
\item \textcolor{black}{vectorizer=TfidfVectorizer(sublinear\_tf=True,max\_df=0.5)~~}
\item \textcolor{black}{feature\_train=vectorizer.fit\_transform(feature\_train)~~}
\item \textcolor{black}{feature\_test=vectorizer.transform(feature\_test).toarray()~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{from}}\textcolor{black}{~sklearn~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~tree~~}
\item \textcolor{black}{clf~=~tree.DecisionTreeClassifier(min\_samples\_split=23)~~}
\item \textcolor{black}{clf.fit(feature\_train,label\_train)~~}
\item \textcolor{black}{pred=clf.predict(feature\_test)~~}
\item \textcolor{black}{importances~=~clf.feature\_importances\_~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{import}}\textcolor{black}{~numpy~as~np~~}
\item \textcolor{black}{indices~=~np.argsort(importances)[::-1]~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{}'Feature~Ranking:~'}\textcolor{black}{~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{for}}\textcolor{black}{~i~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{in}}\textcolor{black}{~range(10):~~}
\item
\textcolor{black}{~~~~}\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}\{\}~feature~no.\{\}~(\{\}){\textquotedbl}}\textcolor{black}{.format(i+1,indices[i],importances[indices[i]])~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~vectorizer.get\_feature\_names()[14343]~~}
\item \textcolor{black}{accuracy=accuracy\_score(label\_test,pred)~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}accuracy={\textquotedbl}}\textcolor{black}{,accuracy~~}
\item
\textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{~}\textcolor{blue}{{\textquotedbl}confusion\_matrix:{\textquotedbl}}\textcolor{black}{~~}
\item \textbf{\textcolor[rgb]{0.0,0.4,0.6}{print}}\textcolor{black}{(classification\_report(label\_test,pred))~}
\end{enumerate}

\bigskip

\section[Future scope of the project]{\textbf{Future scope of the project}}
\hypertarget{Toc451594964}{}
\bigskip

In future both bag of words and random forest strategy can be implemented together to further improve the accuracy of
the resulting classifier and then same can be used in MIR systems (Music Identification and Recommender), that can be
incorporated in a music app and can improve user experience.


\bigskip


\bigskip


\bigskip

\clearpage\section[References]{\textbf{References}}
\hypertarget{Toc451594965}{}
\bigskip

[1]\ \ From Classical to Hip-hop: Can Machine Learn Genre? A student publication by Aaron kravitz, Eliza lupone, Ryan
Diaz


\bigskip

[2]\ \ \ \ \textcolor{black}{Music Genre}

\textcolor{black}{\ \ \ \ }\url{https://en.wikipedia.org/wiki/Music_genre}


\bigskip

[3]\ \ \textcolor{black}{Analysis of the organizational and informational value of links in psychology and geology
popular science hyper articles, }

\url{http://www.scielo.cl/scielo.php?pid=S071809342008000300004&script=sci_arttext}


\bigskip

\textcolor{black}{[4]\ \ MSD genre dataset.'' [Online]. Available:
}\href{http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/msd%20genre%20dataset.zip}{\textcolor{black}{http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/msd genre dataset.zip}}


\bigskip

\textcolor{black}{[5]\ \ Lyrics Retrieving }

\textcolor{black}{http://lyrics.wikia.com/}


\bigskip


\bigskip

\textcolor{black}{[6]\ \ Introduction to Machine Learning, }

\url{https://www.udacity.com/course/intro-to-machine-learning--ud120}


\bigskip

\textcolor{black}{[7]\ \ \ \ Preprocessing data}

\textcolor{black}{\ \ \ \ }\url{http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing}


\bigskip

\textcolor{black}{[8]\ \ \ \ Python Libraries}

\textcolor{black}{\ \ \ \ }\url{http://scikit-learn.org/stable/tutorial/basic/tutorial.html}


\bigskip
\end{document}
